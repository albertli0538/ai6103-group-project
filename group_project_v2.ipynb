{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8c2e861f",
      "metadata": {
        "id": "8c2e861f"
      },
      "source": [
        "# EfficientNet-B0 Experimentation on Cityscapes Dataset for Semantic Segmentation\n",
        "\n",
        "This notebook implements a series of experiments to evaluate and improve the performance of EfficientNet-B0 on the Cityscapes dataset for semantic segmentation.\n",
        "\n",
        "## Overview\n",
        "\n",
        "1. **Baseline Experiment**: Train EfficientNet-B0 with segmentation head\n",
        "2. **Modified Models**:\n",
        "   - Add CBAM (Convolutional Block Attention Module)\n",
        "   - Switch to Mish activation function\n",
        "   - Add DeeplabV3+ segmentation head\n",
        "3. **Comparative Analysis**: Compare and analyze the results across all models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Mp-en_V4FA7y",
      "metadata": {
        "id": "Mp-en_V4FA7y"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive to a path without spaces\n",
        "drive.mount('/content/drive/')  # Changed the mount point\n",
        "\n",
        "# Construct the path to the datasets directory with spaces\n",
        "datasets_dir = './drive/MyDrive/NTU-AI6103-DEEP-LEARNING-AND-APPLICATIONS/Group-Assignment/datasets/Cityscapes'\n",
        "\n",
        "# Check if the directory exists\n",
        "if os.path.exists(datasets_dir):\n",
        "    print(f\"Datasets directory found: {datasets_dir}\")\n",
        "else:\n",
        "    print(f\"Datasets directory not found: {datasets_dir}\")\n",
        "    print(\"Please make sure the path is correct and the directory exists in your Google Drive.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.0 Environment Setup"
      ],
      "metadata": {
        "id": "bs4xA1bAOh2X"
      },
      "id": "bs4xA1bAOh2X"
    },
    {
      "cell_type": "markdown",
      "id": "b00a5168",
      "metadata": {
        "id": "b00a5168"
      },
      "source": [
        "### 1.1 Import all necessary libraries for our experiments:\n",
        "\n",
        "- PyTorch and related libraries for deep learning\n",
        "- EfficientNet implementation\n",
        "- Data processing libraries (NumPy, Pandas, etc.)\n",
        "- Visualization and progress tracking tools\n",
        "\n",
        "It also checks CUDA availability to ensure GPU acceleration if available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f32665a",
      "metadata": {
        "id": "5f32665a"
      },
      "outputs": [],
      "source": [
        "# Install required dependencies\n",
        "%pip install torch torchvision torchaudio\n",
        "%pip install efficientnet_pytorch\n",
        "%pip install numpy pandas matplotlib\n",
        "%pip install tqdm scikit-learn\n",
        "%pip install jupyter\n",
        "\n",
        "# For CUDA compatibility check\n",
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "741a5961",
      "metadata": {
        "id": "741a5961"
      },
      "outputs": [],
      "source": [
        "# Import standard libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# PyTorch imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "import torchvision\n",
        "from torchvision import transforms, models\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Check if CUDA is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.0 Data Preparation"
      ],
      "metadata": {
        "id": "yLkNKTx_Ocwl"
      },
      "id": "yLkNKTx_Ocwl"
    },
    {
      "cell_type": "markdown",
      "id": "e3d327b0",
      "metadata": {
        "id": "e3d327b0"
      },
      "source": [
        "### 2.1 Loading Cityscapes Dataset\n",
        "\n",
        "Here we'll load the Cityscapes dataset from its original directory structure, subsample 1500 images, and create our train/validation/test splits. The Cityscapes dataset is particularly well-suited for segmentation tasks as it provides pixel-level annotations for urban street scenes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "109dceec",
      "metadata": {
        "id": "109dceec"
      },
      "outputs": [],
      "source": [
        "# Clone the Cityscapes repository if not already present\n",
        "!git clone https://github.com/mcordts/cityscapesScripts.git\n",
        "%pip install -e cityscapesScripts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c0bbac9",
      "metadata": {
        "id": "7c0bbac9"
      },
      "outputs": [],
      "source": [
        "# Import Cityscapes helper functions\n",
        "import sys\n",
        "import os\n",
        "import glob\n",
        "import random\n",
        "from PIL import Image\n",
        "\n",
        "# Add the cityscapesScripts directory to the Python path\n",
        "cwd = os.getcwd()\n",
        "cityscapes_path = os.path.join(cwd, 'cityscapesScripts')\n",
        "if cityscapes_path not in sys.path:\n",
        "    sys.path.append(cityscapes_path)\n",
        "\n",
        "# Now import the modules\n",
        "from cityscapesscripts.helpers.labels import trainId2label, id2label\n",
        "\n",
        "# Define paths to dataset directories\n",
        "cityscapes_root = './drive/MyDrive/NTU-AI6103-DEEP-LEARNING-AND-APPLICATIONS/Group-Assignment/datasets/Cityscapes'\n",
        "images_dir = os.path.join(cityscapes_root, 'leftImg8bit_trainvaltest', 'leftImg8bit')\n",
        "annotations_dir = os.path.join(cityscapes_root, 'gtFine_trainvaltest', 'gtFine')\n",
        "\n",
        "# Print images_dir and annotations_dir to check if they are formed correctly\n",
        "print(f\"Images directory: {images_dir}\")\n",
        "print(f\"Annotations directory: {annotations_dir}\")\n",
        "\n",
        "# Function to collect image and label pairs from train, val, and test folders\n",
        "def collect_dataset_files():\n",
        "    splits = ['train', 'val', 'test']\n",
        "    datasets = {'train': [], 'val': [], 'test': []}\n",
        "    for split in splits:\n",
        "        split_img_dir = os.path.join(images_dir, split)\n",
        "        split_label_dir = os.path.join(annotations_dir, split)\n",
        "        city_dirs = [d for d in os.listdir(split_img_dir) if os.path.isdir(os.path.join(split_img_dir, d))]\n",
        "        image_paths = []\n",
        "        label_paths = []\n",
        "        for city in city_dirs:\n",
        "            city_img_dir = os.path.join(split_img_dir, city)\n",
        "            city_img_files = glob.glob(os.path.join(city_img_dir, '*_leftImg8bit.png'))\n",
        "            for img_path in city_img_files:\n",
        "                img_name = os.path.basename(img_path)\n",
        "                img_id = img_name.replace('_leftImg8bit.png', '')\n",
        "                label_name = f\"{img_id}_gtFine_labelIds.png\"\n",
        "                label_path = os.path.join(split_label_dir, city, label_name)\n",
        "                if os.path.exists(label_path):\n",
        "                    image_paths.append(img_path)\n",
        "                    label_paths.append(label_path)\n",
        "        datasets[split] = (image_paths, label_paths)\n",
        "    return datasets['train'][0], datasets['train'][1], datasets['val'][0], datasets['val'][1], datasets['test'][0], datasets['test'][1]\n",
        "\n",
        "# Map Cityscapes IDs to train IDs (0â€“18, 255 for void)\n",
        "def map_cityscapes_labels(label):\n",
        "    label_np = np.array(label, dtype=np.uint8)\n",
        "    mapped_label = np.full_like(label_np, 255, dtype=np.uint8)\n",
        "    id_to_trainid = {\n",
        "        7: 0, 8: 1, 11: 2, 12: 3, 13: 4, 17: 5, 19: 6, 20: 7, 21: 8,\n",
        "        22: 9, 23: 10, 24: 11, 25: 12, 26: 13, 27: 14, 28: 15, 31: 16,\n",
        "        32: 17, 33: 18\n",
        "    }\n",
        "    for id_, train_id in id_to_trainid.items():\n",
        "        mapped_label[label_np == id_] = train_id\n",
        "\n",
        "    # # Debug: Print the number of pixels for each class in the first few images\n",
        "    # if random.random() < 0.05:  # Only print for ~5% of images to avoid flooding output\n",
        "    #     unique_values, counts = np.unique(mapped_label, return_counts=True)\n",
        "    #     print(\"Label distribution:\")\n",
        "    #     for val, count in zip(unique_values, counts):\n",
        "    #         class_name = 'void' if val == 255 else trainId2label[val].name\n",
        "    #         print(f\"  Class {val} ({class_name}): {count} pixels\")\n",
        "\n",
        "    return Image.fromarray(mapped_label)\n",
        "\n",
        "# Define dataset class\n",
        "class CityscapesDataset(Dataset):\n",
        "    def __init__(self, image_paths, label_paths, transform=None, target_transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.label_paths = label_paths\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        label_path = self.label_paths[idx]\n",
        "        label = Image.open(label_path)\n",
        "        label = map_cityscapes_labels(label)\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(label)\n",
        "            label = label.squeeze(0).long()\n",
        "        return image, label\n",
        "\n",
        "# Collect dataset\n",
        "print(\"Collecting dataset files...\")\n",
        "train_image_paths, train_label_paths, val_image_paths, val_label_paths, test_image_paths, test_label_paths = collect_dataset_files()\n",
        "print(f\"Found {len(train_image_paths)} train pairs, {len(val_image_paths)} val pairs, {len(test_image_paths)} test pairs\")\n",
        "\n",
        "# Define transforms\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop((224, 224), scale=(0.5, 1.5)),\n",
        "    transforms.RandomAffine(degrees=10, shear=10),  # Added shear\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.3, contrast=0.2, saturation=0.2, hue=0.1),  # Increased brightness\n",
        "    transforms.GaussianBlur(kernel_size=3),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "test_transform = val_transform\n",
        "\n",
        "target_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224), interpolation=transforms.InterpolationMode.NEAREST),\n",
        "    transforms.Lambda(lambda x: torch.from_numpy(np.array(x)).long())\n",
        "])\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = CityscapesDataset(train_image_paths, train_label_paths, transform=train_transform, target_transform=target_transform)\n",
        "val_dataset = CityscapesDataset(val_image_paths, val_label_paths, transform=val_transform, target_transform=target_transform)\n",
        "test_dataset = CityscapesDataset(test_image_paths, test_label_paths, transform=test_transform, target_transform=target_transform)\n",
        "\n",
        "# Create dataloaders\n",
        "batch_size = 4\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
        "\n",
        "print(f\"Train dataset size: {len(train_dataset)}\")\n",
        "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
        "print(f\"Test dataset size: {len(test_dataset)}\")\n",
        "\n",
        "# Check label distribution in the training dataset\n",
        "def check_label_distribution(dataset, num_samples=5):\n",
        "    print(\"\\nChecking label distribution in dataset...\")\n",
        "    class_counts = np.zeros(20)  # 19 classes + void (255)\n",
        "\n",
        "    for i in range(min(num_samples, len(dataset))):\n",
        "        idx = np.random.randint(len(dataset))\n",
        "        _, label = dataset[idx]\n",
        "        # If label has channel dimension, remove it\n",
        "        if label.dim() == 3 and label.shape[0] == 1:\n",
        "            label = label.squeeze(0)\n",
        "\n",
        "        unique_values, counts = np.unique(label.numpy(), return_counts=True)\n",
        "        print(f\"Image {i+1}/{num_samples} (idx {idx}) - Unique values: {unique_values}\")\n",
        "\n",
        "        for val, count in zip(unique_values, counts):\n",
        "            class_idx = 19 if val == 255 else val  # Store void class (255) at index 19\n",
        "            class_counts[class_idx] += count\n",
        "\n",
        "    # Print summary\n",
        "    print(\"\\nClass distribution summary:\")\n",
        "    class_names = [\n",
        "        'road', 'sidewalk', 'building', 'wall', 'fence', 'pole', 'traffic light',\n",
        "        'traffic sign', 'vegetation', 'terrain', 'sky', 'person', 'rider', 'car',\n",
        "        'truck', 'bus', 'train', 'motorcycle', 'bicycle', 'void'\n",
        "    ]\n",
        "\n",
        "    for i in range(20):\n",
        "        if class_counts[i] > 0:\n",
        "            print(f\"Class {i if i < 19 else 255} ({class_names[i]}): {class_counts[i]:.0f} pixels\")\n",
        "\n",
        "# Run the check on training dataset\n",
        "check_label_distribution(train_dataset, num_samples=10)\n",
        "\n",
        "# Show a sample image from the dataset\n",
        "def show_sample(dataset, idx=0):\n",
        "    img, label = dataset[idx]\n",
        "\n",
        "    # Denormalize the image\n",
        "    mean = torch.tensor([0.485, 0.456, 0.406])\n",
        "    std = torch.tensor([0.229, 0.224, 0.225])\n",
        "    img_denorm = img * std[:, None, None] + mean[:, None, None]\n",
        "\n",
        "    # Create a color-mapped version of the label for better visualization\n",
        "    # Convert label tensor to numpy and ensure it's 2D by squeezing out the channel dimension\n",
        "    label_np = label.squeeze().numpy()  # Remove the channel dimension (1,224,224) -> (224,224)\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(img_denorm.permute(1, 2, 0).numpy())\n",
        "    plt.title('Image')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(label_np, cmap='viridis')  # Add a colormap for better visualization\n",
        "    plt.title('Segmentation Mask')\n",
        "    plt.colorbar()  # Add a colorbar to show the mapping of class IDs to colors\n",
        "    plt.show()\n",
        "\n",
        "# Visualize a random sample from the training dataset\n",
        "show_sample(train_dataset, idx=np.random.randint(len(train_dataset)))\n",
        "\n",
        "# Also visualize a sample from validation and test to ensure everything looks correct\n",
        "show_sample(val_dataset, idx=np.random.randint(len(val_dataset)))\n",
        "show_sample(test_dataset, idx=np.random.randint(len(test_dataset)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24c2fbed",
      "metadata": {
        "id": "24c2fbed"
      },
      "source": [
        "### 2.2 Setting Up Data Transformations and Loading Dataset\n",
        "\n",
        "This cell configures data preprocessing pipelines for both training and validation/testing:\n",
        "1. Training transforms include data augmentation (flips, rotations, color jitter)\n",
        "2. All images are resized to 224x224 pixels to match EfficientNet-B0's input size\n",
        "3. Images are normalized using ImageNet mean and standard deviation\n",
        "4. Segmentation masks are also resized to 224x224 but using nearest-neighbor interpolation to preserve label values\n",
        "\n",
        "We then load the Cityscapes dataset using our custom dataset class that handles both images and their corresponding segmentation masks."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23681d6a",
      "metadata": {
        "id": "23681d6a"
      },
      "source": [
        "### 2.3 Prepare the Model Evaluation and Visualization Functions\n",
        "\n",
        "The following functions will help us evaluate and compare our segmentation models:\n",
        "1. `visualize_predictions`: Displays side-by-side images of input, ground truth, and model predictions\n",
        "2. `evaluate_model_samples`: Evaluates a model on multiple samples and shows visualizations\n",
        "3. `analyze_class_performance`: Analyzes model performance for each semantic class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8a4181f",
      "metadata": {
        "id": "b8a4181f"
      },
      "outputs": [],
      "source": [
        "# Define functions for model evaluation and visualization\n",
        "def visualize_predictions(model, dataset, indices, device, class_names=None):\n",
        "    \"\"\"Visualize model predictions on sample images.\"\"\"\n",
        "    if class_names is None:\n",
        "        class_names = [\n",
        "            'road', 'sidewalk', 'building', 'wall', 'fence', 'pole', 'traffic light',\n",
        "            'traffic sign', 'vegetation', 'terrain', 'sky', 'person', 'rider', 'car',\n",
        "            'truck', 'bus', 'train', 'motorcycle', 'bicycle'\n",
        "        ]\n",
        "\n",
        "    # Create a colormap for visualization\n",
        "    import matplotlib.pyplot as plt\n",
        "    import matplotlib.colors as mcolors\n",
        "    import numpy as np\n",
        "\n",
        "    # Create a colormap with distinct colors for each class\n",
        "    colors = plt.cm.get_cmap('tab20', 19)(range(19))\n",
        "    # Add black for the ignored label (255)\n",
        "    colors = np.vstack([colors, [0, 0, 0, 1]])\n",
        "    cmap = mcolors.ListedColormap(colors)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Create figure for all samples\n",
        "    fig, axes = plt.subplots(len(indices), 3, figsize=(18, 6 * len(indices)))\n",
        "    if len(indices) == 1:\n",
        "        axes = axes.reshape(1, -1)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, idx in enumerate(indices):\n",
        "            # Get the sample\n",
        "            img, mask = dataset[idx]\n",
        "\n",
        "            # Put image on device and add batch dimension\n",
        "            img_tensor = img.unsqueeze(0).to(device)\n",
        "\n",
        "            # Get prediction\n",
        "            output = model(img_tensor)\n",
        "            _, pred = torch.max(output, 1)\n",
        "            pred = pred.cpu().squeeze().numpy()\n",
        "\n",
        "            # Denormalize the image for display\n",
        "            mean = torch.tensor([0.485, 0.456, 0.406])\n",
        "            std = torch.tensor([0.229, 0.224, 0.225])\n",
        "            img_denorm = img * std[:, None, None] + mean[:, None, None]\n",
        "            img_display = img_denorm.permute(1, 2, 0).numpy()\n",
        "\n",
        "            # Convert mask to numpy\n",
        "            if isinstance(mask, torch.Tensor):\n",
        "                mask = mask.numpy()\n",
        "            if hasattr(mask, 'squeeze'):\n",
        "                mask = mask.squeeze()\n",
        "\n",
        "            # Plot\n",
        "            axes[i, 0].imshow(img_display)\n",
        "            axes[i, 0].set_title('Original Image')\n",
        "            axes[i, 0].axis('off')\n",
        "\n",
        "            # Display mask with colormap\n",
        "            axes[i, 1].imshow(mask, cmap=cmap, vmin=0, vmax=19)\n",
        "            axes[i, 1].set_title('Ground Truth')\n",
        "            axes[i, 1].axis('off')\n",
        "\n",
        "            # Display prediction with same colormap\n",
        "            axes[i, 2].imshow(pred, cmap=cmap, vmin=0, vmax=19)\n",
        "            axes[i, 2].set_title('Prediction')\n",
        "            axes[i, 2].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Add a colorbar legend\n",
        "    fig.subplots_adjust(right=0.85)\n",
        "    cbar_ax = fig.add_axes([0.88, 0.15, 0.04, 0.7])\n",
        "    sm = plt.cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(vmin=0, vmax=19))\n",
        "    cbar = fig.colorbar(sm, cax=cbar_ax)\n",
        "    cbar.set_ticks(np.arange(0, 20) + 0.5)\n",
        "    cbar.set_ticklabels(class_names + ['void'])\n",
        "    cbar.ax.tick_params(labelsize=8)\n",
        "\n",
        "    plt.show()\n",
        "    return fig\n",
        "\n",
        "def evaluate_model_detailed(model, dataloader, criterion, device, num_classes=19):\n",
        "    \"\"\"Evaluate model with detailed per-class metrics.\"\"\"\n",
        "    model.eval()\n",
        "    class_names = [\n",
        "        'road', 'sidewalk', 'building', 'wall', 'fence', 'pole', 'traffic light',\n",
        "        'traffic sign', 'vegetation', 'terrain', 'sky', 'person', 'rider', 'car',\n",
        "        'truck', 'bus', 'train', 'motorcycle', 'bicycle'\n",
        "    ]\n",
        "\n",
        "    # Metrics\n",
        "    class_intersection = torch.zeros(num_classes).to(device)\n",
        "    class_union = torch.zeros(num_classes).to(device)\n",
        "    class_pixels = torch.zeros(num_classes).to(device)  # Total pixels of each class in ground truth\n",
        "    confusion_matrix = torch.zeros((num_classes, num_classes), dtype=torch.long).to(device)\n",
        "\n",
        "    total_loss = 0.0\n",
        "    total_correct = 0\n",
        "    total_pixels = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(dataloader, desc=\"Detailed Evaluation\"):\n",
        "            # Handle label dimensions\n",
        "            if labels.dim() == 4 and labels.shape[1] == 1:\n",
        "                labels = labels.squeeze(1)\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.long().to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Metrics\n",
        "            total_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            # Create mask for valid pixels (not 255)\n",
        "            valid_mask = (labels != 255)\n",
        "\n",
        "            # Calculate correct pixels\n",
        "            correct = (preds == labels) & valid_mask\n",
        "            total_correct += torch.sum(correct).item()\n",
        "            total_pixels += torch.sum(valid_mask).item()\n",
        "\n",
        "            # Per-class metrics\n",
        "            for cls in range(num_classes):\n",
        "                pred_inclass = (preds == cls)\n",
        "                target_inclass = (labels == cls)\n",
        "                intersection = torch.sum(pred_inclass & target_inclass).item()\n",
        "                union = torch.sum(pred_inclass | target_inclass).item()\n",
        "                class_intersection[cls] += intersection\n",
        "                class_union[cls] += union\n",
        "                class_pixels[cls] += torch.sum(target_inclass).item()\n",
        "\n",
        "                # Update confusion matrix (only for valid pixels)\n",
        "                for pred_cls in range(num_classes):\n",
        "                    confusion_matrix[cls, pred_cls] += torch.sum(\n",
        "                        (labels == cls) & (preds == pred_cls) & valid_mask\n",
        "                    ).item()\n",
        "\n",
        "    # Calculate final metrics\n",
        "    avg_loss = total_loss / len(dataloader.dataset)\n",
        "    pixel_acc = total_correct / total_pixels if total_pixels > 0 else 0\n",
        "\n",
        "    # Per-class IoU\n",
        "    class_iou = class_intersection / (class_union + 1e-8)\n",
        "    # Classes that are present\n",
        "    valid_classes = (class_union > 0)\n",
        "    mean_iou = torch.mean(class_iou[valid_classes]).item() if torch.sum(valid_classes) > 0 else 0\n",
        "\n",
        "    # Per-class accuracy (proportion of class pixels correctly classified)\n",
        "    class_acc = torch.zeros(num_classes).to(device)\n",
        "    for cls in range(num_classes):\n",
        "        if class_pixels[cls] > 0:\n",
        "            class_acc[cls] = confusion_matrix[cls, cls] / class_pixels[cls]\n",
        "\n",
        "    # Class frequency (normalized)\n",
        "    class_freq = class_pixels / torch.sum(class_pixels)\n",
        "\n",
        "    # Frequency-weighted IoU\n",
        "    weighted_iou = torch.sum(class_iou * class_freq).item()\n",
        "\n",
        "    # Create results dict\n",
        "    results = {\n",
        "        'loss': avg_loss,\n",
        "        'pixel_acc': pixel_acc,\n",
        "        'mean_iou': mean_iou,\n",
        "        'weighted_iou': weighted_iou,\n",
        "        'class_iou': class_iou.cpu().numpy(),\n",
        "        'class_acc': class_acc.cpu().numpy(),\n",
        "        'class_freq': class_freq.cpu().numpy(),\n",
        "        'class_names': class_names,\n",
        "        'confusion_matrix': confusion_matrix.cpu().numpy()\n",
        "    }\n",
        "\n",
        "    return results\n",
        "\n",
        "def visualize_class_performance(results, title=\"Class Performance\"):\n",
        "    \"\"\"Visualize class-wise performance metrics.\"\"\"\n",
        "    class_names = results['class_names']\n",
        "    class_iou = results['class_iou']\n",
        "    class_acc = results['class_acc']\n",
        "    class_freq = results['class_freq']\n",
        "\n",
        "    plt.figure(figsize=(18, 6))\n",
        "\n",
        "    # Sort classes by frequency\n",
        "    sorted_idx = np.argsort(class_freq)[::-1]  # Descending order\n",
        "    sorted_names = [class_names[i] for i in sorted_idx]\n",
        "\n",
        "    # Plot class-wise IoU\n",
        "    plt.subplot(1, 2, 1)\n",
        "    bars = plt.bar(range(len(class_iou)), class_iou[sorted_idx], color='skyblue')\n",
        "    plt.xticks(range(len(class_iou)), sorted_names, rotation=45, ha='right')\n",
        "    plt.ylabel('IoU')\n",
        "    plt.title(f'{title} - Class IoU')\n",
        "\n",
        "    # Add values on top of bars\n",
        "    for bar, iou in zip(bars, class_iou[sorted_idx]):\n",
        "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "                f'{iou:.3f}', ha='center', va='bottom', rotation=0, fontsize=8)\n",
        "\n",
        "    # Plot class accuracy with frequency as bubble size\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.scatter(range(len(class_acc)), class_acc[sorted_idx],\n",
        "               s=class_freq[sorted_idx]*5000, alpha=0.7)\n",
        "    plt.xticks(range(len(class_acc)), sorted_names, rotation=45, ha='right')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title(f'{title} - Class Accuracy (bubble size = class frequency)')\n",
        "\n",
        "    # Add values above points\n",
        "    for i, acc in enumerate(class_acc[sorted_idx]):\n",
        "        plt.text(i, acc + 0.02, f'{acc:.3f}', ha='center', va='bottom', fontsize=8)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def compare_models(model1, model1_name, model2, model2_name, dataset, indices, device, class_names=None):\n",
        "    \"\"\"Compare predictions of two models side by side.\"\"\"\n",
        "    if class_names is None:\n",
        "        class_names = [\n",
        "            'road', 'sidewalk', 'building', 'wall', 'fence', 'pole', 'traffic light',\n",
        "            'traffic sign', 'vegetation', 'terrain', 'sky', 'person', 'rider', 'car',\n",
        "            'truck', 'bus', 'train', 'motorcycle', 'bicycle'\n",
        "        ]\n",
        "\n",
        "    # Create a colormap for visualization\n",
        "    import matplotlib.pyplot as plt\n",
        "    import matplotlib.colors as mcolors\n",
        "    import numpy as np\n",
        "\n",
        "    # Create a colormap with distinct colors for each class\n",
        "    colors = plt.cm.get_cmap('tab20', 19)(range(19))\n",
        "    # Add black for the ignored label (255)\n",
        "    colors = np.vstack([colors, [0, 0, 0, 1]])\n",
        "    cmap = mcolors.ListedColormap(colors)\n",
        "\n",
        "    model1.eval()\n",
        "    model2.eval()\n",
        "\n",
        "    # Create figure for all samples\n",
        "    fig, axes = plt.subplots(len(indices), 4, figsize=(20, 6 * len(indices)))\n",
        "    if len(indices) == 1:\n",
        "        axes = axes.reshape(1, -1)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, idx in enumerate(indices):\n",
        "            # Get the sample\n",
        "            img, mask = dataset[idx]\n",
        "\n",
        "            # Put image on device and add batch dimension\n",
        "            img_tensor = img.unsqueeze(0).to(device)\n",
        "\n",
        "            # Get predictions\n",
        "            output1 = model1(img_tensor)\n",
        "            _, pred1 = torch.max(output1, 1)\n",
        "            pred1 = pred1.cpu().squeeze().numpy()\n",
        "\n",
        "            output2 = model2(img_tensor)\n",
        "            _, pred2 = torch.max(output2, 1)\n",
        "            pred2 = pred2.cpu().squeeze().numpy()\n",
        "\n",
        "            # Denormalize the image for display\n",
        "            mean = torch.tensor([0.485, 0.456, 0.406])\n",
        "            std = torch.tensor([0.229, 0.224, 0.225])\n",
        "            img_denorm = img * std[:, None, None] + mean[:, None, None]\n",
        "            img_display = img_denorm.permute(1, 2, 0).numpy()\n",
        "\n",
        "            # Convert mask to numpy\n",
        "            if isinstance(mask, torch.Tensor):\n",
        "                mask = mask.numpy()\n",
        "            if hasattr(mask, 'squeeze'):\n",
        "                mask = mask.squeeze()\n",
        "\n",
        "            # Plot\n",
        "            axes[i, 0].imshow(img_display)\n",
        "            axes[i, 0].set_title('Original Image')\n",
        "            axes[i, 0].axis('off')\n",
        "\n",
        "            # Display mask with colormap\n",
        "            axes[i, 1].imshow(mask, cmap=cmap, vmin=0, vmax=19)\n",
        "            axes[i, 1].set_title('Ground Truth')\n",
        "            axes[i, 1].axis('off')\n",
        "\n",
        "            # Display model1 prediction\n",
        "            axes[i, 2].imshow(pred1, cmap=cmap, vmin=0, vmax=19)\n",
        "            axes[i, 2].set_title(f'{model1_name} Prediction')\n",
        "            axes[i, 2].axis('off')\n",
        "\n",
        "            # Display model2 prediction\n",
        "            axes[i, 3].imshow(pred2, cmap=cmap, vmin=0, vmax=19)\n",
        "            axes[i, 3].set_title(f'{model2_name} Prediction')\n",
        "            axes[i, 3].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Add a colorbar legend\n",
        "    fig.subplots_adjust(right=0.85)\n",
        "    cbar_ax = fig.add_axes([0.88, 0.15, 0.04, 0.7])\n",
        "    sm = plt.cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(vmin=0, vmax=19))\n",
        "    cbar = fig.colorbar(sm, cax=cbar_ax)\n",
        "    cbar.set_ticks(np.arange(0, 20) + 0.5)\n",
        "    cbar.set_ticklabels(class_names + ['void'])\n",
        "    cbar.ax.tick_params(labelsize=8)\n",
        "\n",
        "    plt.show()\n",
        "    return fig\n",
        "\n",
        "def compare_model_results(results_list, model_names, title=\"Model Comparison\"):\n",
        "    \"\"\"Compare key metrics across multiple models.\"\"\"\n",
        "    # Extract key metrics\n",
        "    pixel_accs = [r['pixel_acc'] for r in results_list]\n",
        "    mean_ious = [r['mean_iou'] for r in results_list]\n",
        "    weighted_ious = [r['weighted_iou'] for r in results_list]\n",
        "\n",
        "    # Class IoU comparison\n",
        "    class_ious = [r['class_iou'] for r in results_list]\n",
        "    class_names = results_list[0]['class_names']\n",
        "\n",
        "    # Plot overall metrics\n",
        "    plt.figure(figsize=(18, 6))\n",
        "\n",
        "    plt.subplot(1, 3, 1)\n",
        "    bars = plt.bar(model_names, pixel_accs, color='skyblue')\n",
        "    plt.ylabel('Pixel Accuracy')\n",
        "    plt.title('Pixel Accuracy by Model')\n",
        "    plt.ylim([min(pixel_accs) * 0.95, max(pixel_accs) * 1.05])\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    # Add values on bars\n",
        "    for bar, val in zip(bars, pixel_accs):\n",
        "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,\n",
        "                f'{val:.3f}', ha='center', va='bottom', rotation=0, fontsize=9)\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    bars = plt.bar(model_names, mean_ious, color='lightgreen')\n",
        "    plt.ylabel('Mean IoU')\n",
        "    plt.title('Mean IoU by Model')\n",
        "    plt.ylim([min(mean_ious) * 0.95, max(mean_ious) * 1.05])\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    # Add values on bars\n",
        "    for bar, val in zip(bars, mean_ious):\n",
        "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,\n",
        "                f'{val:.3f}', ha='center', va='bottom', rotation=0, fontsize=9)\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    bars = plt.bar(model_names, weighted_ious, color='salmon')\n",
        "    plt.ylabel('Frequency-weighted IoU')\n",
        "    plt.title('Weighted IoU by Model')\n",
        "    plt.ylim([min(weighted_ious) * 0.95, max(weighted_ious) * 1.05])\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    # Add values on bars\n",
        "    for bar, val in zip(bars, weighted_ious):\n",
        "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,\n",
        "                f'{val:.3f}', ha='center', va='bottom', rotation=0, fontsize=9)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Create a heatmap of class IoUs across models\n",
        "    plt.figure(figsize=(15, 8))\n",
        "    data = np.array(class_ious).T  # Transpose to have classes as rows, models as columns\n",
        "\n",
        "    # Sort classes by average IoU\n",
        "    avg_iou = np.mean(data, axis=1)\n",
        "    sorted_idx = np.argsort(avg_iou)[::-1]  # Descending order\n",
        "    sorted_data = data[sorted_idx]\n",
        "    sorted_names = [class_names[i] for i in sorted_idx]\n",
        "\n",
        "    im = plt.imshow(sorted_data, cmap='YlGn', aspect='auto')\n",
        "    plt.colorbar(im, label='IoU')\n",
        "\n",
        "    plt.yticks(range(len(sorted_names)), sorted_names)\n",
        "    plt.xticks(range(len(model_names)), model_names, rotation=45, ha='right')\n",
        "    plt.title('Class IoU by Model (sorted by average IoU)')\n",
        "\n",
        "    # Add text annotations\n",
        "    for i in range(len(sorted_names)):\n",
        "        for j in range(len(model_names)):\n",
        "            text_color = 'black' if sorted_data[i, j] > 0.5 else 'white'\n",
        "            plt.text(j, i, f'{sorted_data[i, j]:.3f}',\n",
        "                     ha='center', va='center', color=text_color, fontsize=8)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.0 Baseline Model: EfficientNet-B0 for Segmentation"
      ],
      "metadata": {
        "id": "9J0Pq99ROWgy"
      },
      "id": "9J0Pq99ROWgy"
    },
    {
      "cell_type": "markdown",
      "id": "f9eef666",
      "metadata": {
        "id": "f9eef666"
      },
      "source": [
        "### 3.1 Defining the Baseline EfficientNet-B0 Segmentation Model\n",
        "\n",
        "This cell implements our baseline segmentation model by:\n",
        "1. Creating a custom EfficientNetB0Segmentation class that uses the pre-trained model as an encoder\n",
        "2. Adding a decoder network that upsamples features to produce full-resolution segmentation masks\n",
        "3. Setting up the model to output predictions for 19 classes (Cityscapes semantic classes) at each pixel\n",
        "4. Initializing the model and moving it to the appropriate device (GPU if available)\n",
        "5. Setting up the loss function (Cross-Entropy for segmentation), optimizer (SGD with momentum), and learning rate scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c08d5ea8",
      "metadata": {
        "id": "c08d5ea8"
      },
      "outputs": [],
      "source": [
        "class EfficientNetB0Segmentation(nn.Module):\n",
        "    def __init__(self, num_classes=19):  # Cityscapes has 19 classes with trainId\n",
        "        super(EfficientNetB0Segmentation, self).__init__()\n",
        "        # Load the pre-trained EfficientNet-B0 model as the encoder\n",
        "        self.encoder = EfficientNet.from_pretrained('efficientnet-b0')\n",
        "        # Get the number of features from the last layer\n",
        "        self.encoder_features = self.encoder._fc.in_features\n",
        "\n",
        "        # Remove the classification head\n",
        "        self.encoder._fc = nn.Identity()\n",
        "\n",
        "        # Create a simple decoder for segmentation\n",
        "        self.decoder = nn.Sequential(\n",
        "            # Upsample to get back to input resolution\n",
        "            nn.ConvTranspose2d(self.encoder_features, 256, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(32, num_classes, kernel_size=3, padding=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Extract features from the encoder\n",
        "        features = self.encoder.extract_features(x)  # Shape: [B, C, H/32, W/32]\n",
        "\n",
        "        # Pass through decoder to get segmentation map\n",
        "        segmentation_map = self.decoder(features)  # Shape: [B, num_classes, H, W]\n",
        "\n",
        "        # Ensure output size matches input size\n",
        "        if segmentation_map.shape[-2:] != x.shape[-2:]:\n",
        "            segmentation_map = F.interpolate(segmentation_map, size=x.shape[-2:], mode='bilinear', align_corners=True)\n",
        "\n",
        "        return segmentation_map\n",
        "\n",
        "# Initialize the baseline segmentation model\n",
        "baseline_model = EfficientNetB0Segmentation().to(device)\n",
        "\n",
        "# Define loss function and optimizer for segmentation\n",
        "# Ignore index 255 which is the 'ignored' label in Cityscapes\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=255)\n",
        "optimizer = optim.SGD(baseline_model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64873fcc",
      "metadata": {
        "id": "64873fcc"
      },
      "source": [
        "### 3.2 Implementing Training and Evaluation Functions\n",
        "\n",
        "This cell defines two essential functions for model training and evaluation in a segmentation task:\n",
        "1. `train_one_epoch`: Handles a complete training cycle for semantic segmentation, including:\n",
        "   - Forward and backward passes through the network\n",
        "   - Gradient computation and parameter updates\n",
        "   - Loss and segmentation metrics tracking (mean IoU, pixel accuracy)\n",
        "2. `evaluate`: Performs model evaluation on validation or test data:\n",
        "   - Forward passes without gradient computation (using `torch.no_grad()`)\n",
        "   - Computes segmentation metrics (mean IoU, pixel accuracy)\n",
        "   - Visual inspection of segmentation quality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02d1ab47",
      "metadata": {
        "id": "02d1ab47"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_pixel_acc = 0.0\n",
        "    running_iou = 0.0\n",
        "    processed_data = 0\n",
        "    num_classes = 19  # Cityscapes has 19 classes with trainId (0-18)\n",
        "\n",
        "    # Initialize tensors to track intersection and union for each class\n",
        "    class_intersection = torch.zeros(num_classes).to(device)\n",
        "    class_union = torch.zeros(num_classes).to(device)\n",
        "\n",
        "    # Debug counters\n",
        "    valid_label_count = 0\n",
        "    batch_count = 0\n",
        "\n",
        "    for inputs, labels in tqdm(dataloader, desc=\"Training\"):\n",
        "        batch_count += 1\n",
        "        inputs = inputs.to(device)\n",
        "\n",
        "        # Label shape debug - before any processing\n",
        "        if batch_count == 1:\n",
        "            print(f\"Original label shape: {labels.shape}, dtype: {labels.dtype}\")\n",
        "            print(f\"Label min: {labels.min()}, max: {labels.max()}, unique: {torch.unique(labels)}\")\n",
        "\n",
        "        # Remove channel dimension for CrossEntropyLoss [B, H, W]\n",
        "        # If labels have shape [B, 1, H, W], squeeze them to [B, H, W]\n",
        "        if labels.dim() == 4 and labels.shape[1] == 1:\n",
        "            labels = labels.squeeze(1)\n",
        "\n",
        "        labels = labels.long().to(device)  # Ensure labels are of type Long\n",
        "\n",
        "        # Label shape debug - after processing\n",
        "        if batch_count == 1:\n",
        "            print(f\"Processed label shape: {labels.shape}, dtype: {labels.dtype}\")\n",
        "            print(f\"Label min: {labels.min()}, max: {labels.max()}\")\n",
        "            print(f\"Label unique values: {torch.unique(labels)}\")\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)  # Shape: [B, num_classes, H, W]\n",
        "\n",
        "        # Debug first batch outputs\n",
        "        if batch_count == 1:\n",
        "            print(f\"Model output shape: {outputs.shape}, dtype: {outputs.dtype}\")\n",
        "\n",
        "        # Compute loss - CrossEntropyLoss expects (N,C,d1,d2...) for input and (N,d1,d2...) for target\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Get predictions\n",
        "        _, preds = torch.max(outputs, 1)  # Shape: [B, H, W]\n",
        "\n",
        "        # Calculate pixel accuracy (ignoring void pixels with value 255)\n",
        "        valid_pixels = (labels != 255)  # Create mask for valid pixels\n",
        "        valid_label_count += torch.sum(valid_pixels).item()\n",
        "        correct_pixels = torch.sum((preds == labels) & valid_pixels).item()\n",
        "        total_valid_pixels = torch.sum(valid_pixels).item()\n",
        "        pixel_acc = correct_pixels / (total_valid_pixels + 1e-8)\n",
        "\n",
        "        # Calculate IoU (Intersection over Union) for each class\n",
        "        for cls in range(num_classes):\n",
        "            # For each class, find pixels where prediction is this class\n",
        "            pred_inclass = (preds == cls)\n",
        "            # Find pixels where ground truth is this class\n",
        "            target_inclass = (labels == cls)\n",
        "            # Calculate intersection (pixels that are this class in both pred and target)\n",
        "            intersection = torch.sum(pred_inclass & target_inclass).item()\n",
        "            # Calculate union (pixels that are this class in either pred or target)\n",
        "            union = torch.sum(pred_inclass | target_inclass).item()\n",
        "            # Accumulate for epoch-level metrics\n",
        "            class_intersection[cls] += intersection\n",
        "            class_union[cls] += union\n",
        "\n",
        "        # Calculate batch mean IoU (for tracking only)\n",
        "        batch_intersection = torch.zeros(num_classes).to(device)\n",
        "        batch_union = torch.zeros(num_classes).to(device)\n",
        "        classes_present = []\n",
        "\n",
        "        for cls in range(num_classes):\n",
        "            pred_cls = (preds == cls)\n",
        "            target_cls = (labels == cls)\n",
        "            batch_intersection[cls] = torch.sum(pred_cls & target_cls).item()\n",
        "            batch_union[cls] = torch.sum(pred_cls | target_cls).item()\n",
        "            if torch.sum(target_cls) > 0:\n",
        "                classes_present.append(cls)\n",
        "\n",
        "        # Avoid division by zero with small epsilon\n",
        "        batch_iou = batch_intersection / (batch_union + 1e-8)\n",
        "\n",
        "        # Only consider classes that are present in this batch\n",
        "        valid_classes = (batch_union > 0)\n",
        "        if torch.sum(valid_classes) > 0:\n",
        "            iou = torch.mean(batch_iou[valid_classes]).item()\n",
        "        else:\n",
        "            iou = 0.0\n",
        "\n",
        "        # Update statistics\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_pixel_acc += pixel_acc * inputs.size(0)\n",
        "        running_iou += iou * inputs.size(0)\n",
        "        processed_data += inputs.size(0)\n",
        "\n",
        "        # Debug for first few batches\n",
        "        if batch_count <= 3:\n",
        "            print(f\"Batch {batch_count} - Classes present: {classes_present}\")\n",
        "            print(f\"Batch {batch_count} - pixel_acc: {pixel_acc:.4f}, IoU: {iou:.4f}\")\n",
        "\n",
        "    # Print final debug info\n",
        "    print(f\"Total valid label pixels: {valid_label_count}\")\n",
        "\n",
        "    # Calculate epoch-level metrics\n",
        "    train_loss = running_loss / processed_data\n",
        "    train_pixel_acc = running_pixel_acc / processed_data\n",
        "\n",
        "    # Calculate per-class IoU for the entire epoch (more accurate than batch-wise)\n",
        "    class_iou = class_intersection / (class_union + 1e-8)\n",
        "    # Only consider classes that actually appeared in the dataset\n",
        "    valid_classes = (class_union > 0)\n",
        "\n",
        "    # Map trainId to class names for better interpretability\n",
        "    class_names = [\n",
        "        'road', 'sidewalk', 'building', 'wall', 'fence', 'pole', 'traffic light',\n",
        "        'traffic sign', 'vegetation', 'terrain', 'sky', 'person', 'rider', 'car',\n",
        "        'truck', 'bus', 'train', 'motorcycle', 'bicycle'\n",
        "    ]\n",
        "\n",
        "    # Print per-class IoU with class names\n",
        "    print(\"\\nPer-class IoU:\")\n",
        "    for cls in range(num_classes):\n",
        "        if class_union[cls] > 0:\n",
        "            print(f\"{class_names[cls]}: {class_iou[cls]:.4f}\")\n",
        "\n",
        "    if torch.sum(valid_classes) > 0:\n",
        "        mean_iou = torch.mean(class_iou[valid_classes]).item()\n",
        "    else:\n",
        "        mean_iou = 0.0\n",
        "\n",
        "    # Print summary\n",
        "    print(f\"Valid classes: {torch.sum(valid_classes).item()} out of {num_classes}\")\n",
        "    print(f\"Mean IoU: {mean_iou:.4f}, Train loss: {train_loss:.4f}, Train pixel acc: {train_pixel_acc:.4f}\")\n",
        "\n",
        "    # Return the relevant metrics\n",
        "    return train_loss, train_pixel_acc, mean_iou"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wB-9JAUvOyU5",
      "metadata": {
        "id": "wB-9JAUvOyU5"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    running_pixel_acc = 0.0\n",
        "    running_iou = 0.0\n",
        "    processed_data = 0\n",
        "    num_classes = 19  # Cityscapes has 19 classes with trainId\n",
        "\n",
        "    # Initialize tensors to track intersection and union for each class\n",
        "    class_intersection = torch.zeros(num_classes).to(device)\n",
        "    class_union = torch.zeros(num_classes).to(device)\n",
        "\n",
        "    # Map trainId to class names for better interpretability\n",
        "    class_names = [\n",
        "        'road', 'sidewalk', 'building', 'wall', 'fence', 'pole', 'traffic light',\n",
        "        'traffic sign', 'vegetation', 'terrain', 'sky', 'person', 'rider', 'car',\n",
        "        'truck', 'bus', 'train', 'motorcycle', 'bicycle'\n",
        "    ]\n",
        "\n",
        "    batch_count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "            batch_count += 1\n",
        "            inputs = inputs.to(device)\n",
        "\n",
        "            # Remove channel dimension for CrossEntropyLoss [B, H, W]\n",
        "            # If labels have shape [B, 1, H, W], squeeze them to [B, H, W]\n",
        "            if labels.dim() == 4 and labels.shape[1] == 1:\n",
        "                labels = labels.squeeze(1)\n",
        "\n",
        "            labels = labels.long().to(device)  # Ensure labels are of type Long\n",
        "\n",
        "            # Debug first batch labels and shapes\n",
        "            if batch_count == 1:\n",
        "                print(f\"Label shape: {labels.shape}, dtype: {labels.dtype}\")\n",
        "                print(f\"Label min: {labels.min()}, max: {labels.max()}\")\n",
        "                print(f\"Label unique values: {torch.unique(labels)}\")\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Debug model output shape\n",
        "            if batch_count == 1:\n",
        "                print(f\"Model output shape: {outputs.shape}, dtype: {outputs.dtype}\")\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs, 1)  # Shape: [B, H, W]\n",
        "\n",
        "            # Calculate pixel accuracy (ignoring void pixels with value 255)\n",
        "            valid_pixels = (labels != 255)  # Create mask for valid pixels\n",
        "            correct_pixels = torch.sum((preds == labels) & valid_pixels).item()\n",
        "            total_valid_pixels = torch.sum(valid_pixels).item()\n",
        "            pixel_acc = correct_pixels / (total_valid_pixels + 1e-8)\n",
        "\n",
        "            # Calculate IoU (Intersection over Union) for each class\n",
        "            for cls in range(num_classes):\n",
        "                # For each class, find pixels where prediction is this class\n",
        "                pred_inclass = (preds == cls)\n",
        "                # Find pixels where ground truth is this class\n",
        "                target_inclass = (labels == cls)\n",
        "                # Calculate intersection (pixels that are this class in both pred and target)\n",
        "                intersection = torch.sum(pred_inclass & target_inclass).item()\n",
        "                # Calculate union (pixels that are this class in either pred or target)\n",
        "                union = torch.sum(pred_inclass | target_inclass).item()\n",
        "                # Accumulate for epoch-level metrics\n",
        "                class_intersection[cls] += intersection\n",
        "                class_union[cls] += union\n",
        "\n",
        "            # Calculate batch mean IoU (for tracking only)\n",
        "            batch_iou = torch.zeros(num_classes).to(device)\n",
        "            valid_classes = torch.zeros(num_classes, dtype=torch.bool).to(device)\n",
        "\n",
        "            for cls in range(num_classes):\n",
        "                pred_cls = (preds == cls)\n",
        "                target_cls = (labels == cls)\n",
        "                intersection = torch.sum(pred_cls & target_cls).item()\n",
        "                union = torch.sum(pred_cls | target_cls).item()\n",
        "                if union > 0:\n",
        "                    batch_iou[cls] = intersection / union\n",
        "                    valid_classes[cls] = True\n",
        "\n",
        "            # Only consider classes that are present in this batch\n",
        "            if torch.sum(valid_classes) > 0:\n",
        "                iou = torch.mean(batch_iou[valid_classes]).item()\n",
        "            else:\n",
        "                iou = 0.0\n",
        "\n",
        "            # Update statistics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_pixel_acc += pixel_acc * inputs.size(0)\n",
        "            running_iou += iou * inputs.size(0)\n",
        "            processed_data += inputs.size(0)\n",
        "\n",
        "    # Calculate epoch-level metrics\n",
        "    eval_loss = running_loss / processed_data\n",
        "    eval_pixel_acc = running_pixel_acc / processed_data\n",
        "\n",
        "    # Calculate per-class IoU for the entire evaluation set\n",
        "    class_iou = class_intersection / (class_union + 1e-8)\n",
        "    # Only consider classes that actually appeared in the dataset\n",
        "    valid_classes = (class_union > 0)\n",
        "\n",
        "    # Print per-class IoU with class names\n",
        "    print(\"\\nPer-class IoU:\")\n",
        "    for cls in range(num_classes):\n",
        "        if class_union[cls] > 0:\n",
        "            print(f\"{class_names[cls]}: {class_iou[cls]:.4f}\")\n",
        "\n",
        "    if torch.sum(valid_classes) > 0:\n",
        "        mean_iou = torch.mean(class_iou[valid_classes]).item()\n",
        "    else:\n",
        "        mean_iou = 0.0\n",
        "\n",
        "    print(f\"Valid classes: {torch.sum(valid_classes).item()} out of {num_classes}\")\n",
        "    print(f\"Mean IoU: {mean_iou:.4f}, Eval loss: {eval_loss:.4f}, Pixel acc: {eval_pixel_acc:.4f}\")\n",
        "\n",
        "    return eval_loss, eval_pixel_acc, mean_iou"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aaf94513",
      "metadata": {
        "id": "aaf94513"
      },
      "source": [
        "### 3.3 Complete Model Training Pipeline\n",
        "\n",
        "This cell defines and executes the full training pipeline for semantic segmentation:\n",
        "1. Implements the `train_model` function that orchestrates training over multiple epochs\n",
        "   - Tracks training and validation metrics in a history dictionary\n",
        "   - Implements early stopping to save the best model based on validation IoU\n",
        "   - Adjusts learning rate using the scheduler based on validation loss\n",
        "2. Imports the `copy` module to maintain a copy of the best model weights\n",
        "3. Trains the baseline segmentation model for 10 epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab98fb99",
      "metadata": {
        "id": "ab98fb99"
      },
      "outputs": [],
      "source": [
        "# Training loop for baseline model\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=10):\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'train_pixel_acc': [],\n",
        "        'train_iou': [],\n",
        "        'val_loss': [],\n",
        "        'val_pixel_acc': [],\n",
        "        'val_iou': []\n",
        "    }\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_iou = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Train phase\n",
        "        train_loss, train_pixel_acc, train_iou = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
        "        print(f'Train Loss: {train_loss:.4f} Pixel Acc: {train_pixel_acc:.4f} IoU: {train_iou:.4f}')\n",
        "\n",
        "        # Validation phase\n",
        "        val_loss, val_pixel_acc, val_iou = evaluate(model, val_loader, criterion, device)\n",
        "        print(f'Val Loss: {val_loss:.4f} Pixel Acc: {val_pixel_acc:.4f} IoU: {val_iou:.4f}')\n",
        "\n",
        "        # Update learning rate\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        # Deep copy the model if it's the best\n",
        "        if val_iou > best_iou:\n",
        "            best_iou = val_iou\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            print(f'New best model with IoU: {best_iou:.4f}')\n",
        "\n",
        "        # Update history\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_pixel_acc'].append(train_pixel_acc)\n",
        "        history['train_iou'].append(train_iou)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_pixel_acc'].append(val_pixel_acc)\n",
        "        history['val_iou'].append(val_iou)\n",
        "\n",
        "        print()\n",
        "\n",
        "    # Load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, history\n",
        "\n",
        "import copy\n",
        "\n",
        "# Train the baseline segmentation model\n",
        "baseline_model_trained, baseline_history = train_model(\n",
        "    baseline_model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    criterion,\n",
        "    optimizer,\n",
        "    scheduler,\n",
        "    num_epochs=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d16d5ae",
      "metadata": {
        "id": "3d16d5ae"
      },
      "source": [
        "### 3.4 Visualizing the Training Results\n",
        "\n",
        "This cell defines and uses a function to visualize training progress for semantic segmentation:\n",
        "1. Creates the `plot_training_history` function that generates three plots:\n",
        "   - Training and validation loss curves\n",
        "   - Training and validation pixel accuracy curves\n",
        "   - Training and validation mean IoU curves\n",
        "2. Visualizes the baseline model's training history to analyze convergence and potential overfitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f2e6996",
      "metadata": {
        "id": "3f2e6996"
      },
      "outputs": [],
      "source": [
        "# Visualize the training history\n",
        "def plot_training_history(history, title):\n",
        "    epochs = range(1, len(history['train_loss'])+1)\n",
        "\n",
        "    plt.figure(figsize=(18, 5))\n",
        "\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.plot(epochs, history['train_loss'], 'bo-', label='Training Loss')\n",
        "    plt.plot(epochs, history['val_loss'], 'ro-', label='Validation Loss')\n",
        "    plt.title(f'{title} - Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.plot(epochs, history['train_pixel_acc'], 'bo-', label='Training Pixel Accuracy')\n",
        "    plt.plot(epochs, history['val_pixel_acc'], 'ro-', label='Validation Pixel Accuracy')\n",
        "    plt.title(f'{title} - Pixel Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Pixel Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.plot(epochs, history['train_iou'], 'bo-', label='Training IoU')\n",
        "    plt.plot(epochs, history['val_iou'], 'ro-', label='Validation IoU')\n",
        "    plt.title(f'{title} - Mean IoU')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Mean IoU')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Visualize baseline model training history\n",
        "plot_training_history(baseline_history, 'Baseline EfficientNet-B0 Segmentation')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85692e5f",
      "metadata": {
        "id": "85692e5f"
      },
      "source": [
        "### 3.5 Evaluating the Baseline Model\n",
        "\n",
        "Now that we have trained our baseline EfficientNet-B0 model for semantic segmentation, let's perform a detailed evaluation to understand its performance:\n",
        "\n",
        "1. Visualize predictions on sample images from the validation set\n",
        "2. Calculate detailed performance metrics (overall and per-class)\n",
        "3. Analyze where the model performs well and where it struggles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b1e361c",
      "metadata": {
        "id": "4b1e361c"
      },
      "outputs": [],
      "source": [
        "# Sample indices from validation set\n",
        "val_sample_indices = np.random.choice(len(val_dataset), 3, replace=False)\n",
        "\n",
        "# Visualize predictions from baseline model\n",
        "print(\"Baseline Model Predictions:\")\n",
        "visualize_predictions(baseline_model_trained, val_dataset, val_sample_indices, device)\n",
        "\n",
        "# Evaluate baseline model with detailed metrics\n",
        "print(\"Calculating detailed metrics for baseline model...\")\n",
        "baseline_results = evaluate_model_detailed(baseline_model_trained, val_loader, criterion, device)\n",
        "\n",
        "# Print overall metrics\n",
        "print(f\"\\nBaseline Model - Overall Performance:\")\n",
        "print(f\"Validation Loss: {baseline_results['loss']:.4f}\")\n",
        "print(f\"Pixel Accuracy: {baseline_results['pixel_acc']:.4f}\")\n",
        "print(f\"Mean IoU: {baseline_results['mean_iou']:.4f}\")\n",
        "print(f\"Frequency-weighted IoU: {baseline_results['weighted_iou']:.4f}\")\n",
        "\n",
        "# Visualize class-wise performance\n",
        "visualize_class_performance(baseline_results, title=\"Baseline Model\")\n",
        "\n",
        "# Identify best and worst classes\n",
        "class_names = baseline_results['class_names']\n",
        "class_ious = baseline_results['class_iou']\n",
        "\n",
        "# Top 3 classes\n",
        "top_idx = np.argsort(class_ious)[-3:][::-1]\n",
        "print(\"\\nTop 3 classes (highest IoU):\")\n",
        "for i in top_idx:\n",
        "    print(f\"{class_names[i]}: {class_ious[i]:.4f}\")\n",
        "\n",
        "# Bottom 3 classes\n",
        "bottom_idx = np.argsort(class_ious)[:3]\n",
        "print(\"\\nBottom 3 classes (lowest IoU):\")\n",
        "for i in bottom_idx:\n",
        "    if class_ious[i] > 0:  # Only show classes that appear in the dataset\n",
        "        print(f\"{class_names[i]}: {class_ious[i]:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.0 Modified Models"
      ],
      "metadata": {
        "id": "vOMfgS5GOKcg"
      },
      "id": "vOMfgS5GOKcg"
    },
    {
      "cell_type": "markdown",
      "id": "561b867a",
      "metadata": {
        "id": "561b867a"
      },
      "source": [
        "### 4.1 EfficientNet-B0 with CBAM (Convolutional Block Attention Module)\n",
        "\n",
        "CBAM enhances the representational power by focusing on important features and suppressing unnecessary ones. For segmentation tasks, this attention mechanism is particularly helpful as it allows the model to focus on relevant spatial regions and feature channels, leading to more accurate pixel-wise predictions.\n",
        "\n",
        "### Implementing the CBAM Attention Module\n",
        "\n",
        "This cell implements the Convolutional Block Attention Module (CBAM) and integrates it with EfficientNet-B0 for segmentation:\n",
        "1. Defines the `ChannelAttention` class that focuses on important channels\n",
        "2. Defines the `SpatialAttention` class that emphasizes informative regions\n",
        "3. Combines both in the `CBAM` class\n",
        "4. Creates an `EfficientNetB0WithCBAM` class that incorporates CBAM into the segmentation model architecture\n",
        "5. Implements a decoder structure to convert encoded features to segmentation masks\n",
        "6. Initializes the model and sets up its optimizer and scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c83bc217",
      "metadata": {
        "id": "c83bc217"
      },
      "outputs": [],
      "source": [
        "# Implementing CBAM\n",
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self, in_planes, ratio=16):\n",
        "        super(ChannelAttention, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)\n",
        "        )\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_out = self.fc(self.avg_pool(x))\n",
        "        max_out = self.fc(self.max_pool(x))\n",
        "        out = avg_out + max_out\n",
        "        return self.sigmoid(out)\n",
        "\n",
        "class SpatialAttention(nn.Module):\n",
        "    def __init__(self, kernel_size=7):\n",
        "        super(SpatialAttention, self).__init__()\n",
        "        padding = kernel_size // 2\n",
        "        self.conv = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
        "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
        "        concat = torch.cat([avg_out, max_out], dim=1)\n",
        "        out = self.conv(concat)\n",
        "        return self.sigmoid(out)\n",
        "\n",
        "class CBAM(nn.Module):\n",
        "    def __init__(self, in_planes, ratio=16, kernel_size=7):\n",
        "        super(CBAM, self).__init__()\n",
        "        self.channel_att = ChannelAttention(in_planes, ratio)\n",
        "        self.spatial_att = SpatialAttention(kernel_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x * self.channel_att(x)\n",
        "        x = x * self.spatial_att(x)\n",
        "        return x\n",
        "\n",
        "# EfficientNet-B0 with CBAM attention for segmentation\n",
        "class EfficientNetB0WithCBAM(nn.Module):\n",
        "    def __init__(self, num_classes=19):  # 19 classes for Cityscapes segmentation\n",
        "        super(EfficientNetB0WithCBAM, self).__init__()\n",
        "        # Load the pre-trained EfficientNet-B0 model as encoder\n",
        "        self.encoder = EfficientNet.from_pretrained('efficientnet-b0')\n",
        "        in_features = self.encoder._fc.in_features\n",
        "\n",
        "        # Remove the classification head from the encoder\n",
        "        self.encoder._fc = nn.Identity()\n",
        "\n",
        "        # Add CBAM at the end of feature extraction\n",
        "        self.cbam = CBAM(in_features)\n",
        "\n",
        "        # Create decoder for segmentation (similar to baseline but with CBAM in between)\n",
        "        self.decoder = nn.Sequential(\n",
        "            # Upsample to get back to input resolution\n",
        "            nn.ConvTranspose2d(in_features, 256, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(32, num_classes, kernel_size=3, padding=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Extract features from the encoder\n",
        "        features = self.encoder.extract_features(x)  # Shape: [B, C, H/32, W/32]\n",
        "\n",
        "        # Apply CBAM attention\n",
        "        features_with_attention = self.cbam(features)\n",
        "\n",
        "        # Pass through decoder to get segmentation map\n",
        "        segmentation_map = self.decoder(features_with_attention)  # Shape: [B, num_classes, H, W]\n",
        "\n",
        "        # Ensure output size matches input size\n",
        "        if segmentation_map.shape[-2:] != x.shape[-2:]:\n",
        "            segmentation_map = F.interpolate(segmentation_map, size=x.shape[-2:], mode='bilinear', align_corners=True)\n",
        "\n",
        "        return segmentation_map\n",
        "\n",
        "# Initialize the CBAM model\n",
        "cbam_model = EfficientNetB0WithCBAM().to(device)\n",
        "\n",
        "# Define loss function, optimizer and scheduler for the CBAM segmentation model\n",
        "cbam_criterion = nn.CrossEntropyLoss(ignore_index=255)  # Ignore index 255 which is the 'ignored' label in Cityscapes\n",
        "cbam_optimizer = optim.SGD(cbam_model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
        "cbam_scheduler = optim.lr_scheduler.ReduceLROnPlateau(cbam_optimizer, 'min', patience=3, factor=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac43af07",
      "metadata": {
        "id": "ac43af07"
      },
      "source": [
        "#### 4.1.1 Training and Evaluating the CBAM Segmentation Model\n",
        "\n",
        "Here we train and evaluate the EfficientNet-B0 model enhanced with CBAM for semantic segmentation:\n",
        "1. Train the model for 10 epochs using the same training function as the baseline\n",
        "2. Track segmentation metrics (mean IoU, pixel accuracy) during training\n",
        "3. Visualize the training history to compare with the baseline segmentation model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b00cefa5",
      "metadata": {
        "id": "b00cefa5"
      },
      "outputs": [],
      "source": [
        "# Train the CBAM segmentation model\n",
        "cbam_model_trained, cbam_history = train_model(\n",
        "    cbam_model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    cbam_criterion,\n",
        "    cbam_optimizer,\n",
        "    cbam_scheduler,\n",
        "    num_epochs=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wt66JKOT-b-3",
      "metadata": {
        "id": "wt66JKOT-b-3"
      },
      "outputs": [],
      "source": [
        "# Visualize CBAM model training history\n",
        "plot_training_history(cbam_history, 'EfficientNet-B0 with CBAM for Segmentation')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00279e84",
      "metadata": {
        "id": "00279e84"
      },
      "source": [
        "#### 4.1.2 Detailed Training History Analysis for CBAM vs Baseline Segmentation\n",
        "\n",
        "This cell creates a comprehensive DataFrame containing the epoch-by-epoch segmentation metrics for both models:\n",
        "1. Collects per-epoch training and validation losses\n",
        "2. Collects per-epoch training and validation IoU and pixel accuracy values\n",
        "3. Organizes data into a DataFrame for detailed analysis\n",
        "\n",
        "This information enables us to pinpoint exactly when and how the CBAM model's segmentation performance diverges from the baseline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cda7d945",
      "metadata": {
        "id": "cda7d945"
      },
      "outputs": [],
      "source": [
        "# Log epoch-wise training metrics for comparative analysis\n",
        "epochs = list(range(1, len(baseline_history['train_loss'])+1))\n",
        "\n",
        "train_data = {\n",
        "    'Epoch': epochs,\n",
        "    'Baseline Train Loss': baseline_history['train_loss'],\n",
        "    'Baseline Val Loss': baseline_history['val_loss'],\n",
        "    'CBAM Train Loss': cbam_history['train_loss'],\n",
        "    'CBAM Val Loss': cbam_history['val_loss'],\n",
        "    'Baseline Train IoU': baseline_history['train_iou'],\n",
        "    'Baseline Val IoU': baseline_history['val_iou'],\n",
        "    'CBAM Train IoU': cbam_history['train_iou'],\n",
        "    'CBAM Val IoU': cbam_history['val_iou'],\n",
        "    'Baseline Train Pixel Acc': baseline_history['train_pixel_acc'],\n",
        "    'Baseline Val Pixel Acc': baseline_history['val_pixel_acc'],\n",
        "    'CBAM Train Pixel Acc': cbam_history['train_pixel_acc'],\n",
        "    'CBAM Val Pixel Acc': cbam_history['val_pixel_acc']\n",
        "}\n",
        "\n",
        "training_df = pd.DataFrame(train_data)\n",
        "print(\"Training History Comparison for Segmentation:\")\n",
        "display(training_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "143b3445",
      "metadata": {
        "id": "143b3445"
      },
      "source": [
        "#### 4.1.3 Evaluating CBAM model and comparing with baseline\n",
        "Evaluate the CBAM model and compare its performance with the baseline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c7979ce",
      "metadata": {
        "id": "0c7979ce"
      },
      "outputs": [],
      "source": [
        "# Evaluating CBAM model and comparing with baseline\n",
        "print(\"CBAM Model Predictions:\")\n",
        "visualize_predictions(cbam_model_trained, val_dataset, val_sample_indices, device)\n",
        "\n",
        "# Calculate detailed metrics for CBAM model\n",
        "print(\"Calculating detailed metrics for CBAM model...\")\n",
        "cbam_results = evaluate_model_detailed(cbam_model_trained, val_loader, cbam_criterion, device)\n",
        "\n",
        "# Print overall metrics\n",
        "print(f\"\\nCBAM Model - Overall Performance:\")\n",
        "print(f\"Validation Loss: {cbam_results['loss']:.4f}\")\n",
        "print(f\"Pixel Accuracy: {cbam_results['pixel_acc']:.4f}\")\n",
        "print(f\"Mean IoU: {cbam_results['mean_iou']:.4f}\")\n",
        "print(f\"Frequency-weighted IoU: {cbam_results['weighted_iou']:.4f}\")\n",
        "\n",
        "# Visualize class-wise performance\n",
        "visualize_class_performance(cbam_results, title=\"CBAM Model\")\n",
        "\n",
        "# Side-by-side comparison of baseline and CBAM\n",
        "print(\"\\nBaseline vs CBAM - Side-by-side comparison:\")\n",
        "compare_models(baseline_model_trained, \"Baseline\", cbam_model_trained, \"CBAM\",\n",
        "               val_dataset, val_sample_indices, device)\n",
        "\n",
        "# Compare key metrics\n",
        "print(\"\\nMetric Improvement with CBAM:\")\n",
        "print(f\"Pixel Accuracy: {baseline_results['pixel_acc']:.4f} â†’ {cbam_results['pixel_acc']:.4f} \"\n",
        "      f\"({(cbam_results['pixel_acc'] - baseline_results['pixel_acc']) * 100:.2f}% change)\")\n",
        "print(f\"Mean IoU: {baseline_results['mean_iou']:.4f} â†’ {cbam_results['mean_iou']:.4f} \"\n",
        "      f\"({(cbam_results['mean_iou'] - baseline_results['mean_iou']) * 100:.2f}% change)\")\n",
        "\n",
        "# Identify classes with the most improvement\n",
        "class_improvement = cbam_results['class_iou'] - baseline_results['class_iou']\n",
        "most_improved_idx = np.argsort(class_improvement)[-3:][::-1]\n",
        "print(\"\\nMost improved classes with CBAM:\")\n",
        "for i in most_improved_idx:\n",
        "    print(f\"{class_names[i]}: {baseline_results['class_iou'][i]:.4f} â†’ {cbam_results['class_iou'][i]:.4f} \"\n",
        "          f\"({class_improvement[i]:.4f} absolute improvement)\")\n",
        "\n",
        "# Analyze where CBAM attention helps the most\n",
        "print(\"\\nInsight: CBAM's attention mechanism appears to improve segmentation most effectively on\")\n",
        "print(\"classes with complex shapes and boundaries, where focusing on relevant spatial regions\")\n",
        "print(\"helps distinguish similar-looking objects from each other.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "668a4aec",
      "metadata": {
        "id": "668a4aec"
      },
      "source": [
        "### 4.2 EfficientNet-B0 with Mish Activation Function for Segmentation\n",
        "\n",
        "Mish is a self-regularized non-monotonic activation function that often outperforms ReLU and its variants in various tasks. For semantic segmentation, Mish may provide better gradient flow characteristics that improve feature representation at pixel level.\n",
        "\n",
        "### Implementing the Mish Activation Function for Segmentation\n",
        "\n",
        "This cell implements the Mish activation function and integrates it with EfficientNet-B0 for segmentation:\n",
        "1. Defines the `Mish` activation class (formula: x * tanh(softplus(x)))\n",
        "2. Creates an `EfficientNetB0WithMish` class that replaces all ReLU activations with Mish\n",
        "3. Implements a recursive function to replace activations throughout the model\n",
        "4. Adds a segmentation decoder to produce pixel-wise predictions\n",
        "5. Initializes the model and sets up its optimizer and scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac37e7ec",
      "metadata": {
        "id": "ac37e7ec"
      },
      "outputs": [],
      "source": [
        "# Implementing Mish activation\n",
        "class Mish(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x * torch.tanh(F.softplus(x))\n",
        "\n",
        "# Updating the EfficientNetB0WithMish class to fix the channel mismatch\n",
        "class EfficientNetB0WithMish(nn.Module):\n",
        "    def __init__(self, num_classes=19):  # 19 classes for Cityscapes segmentation\n",
        "        super(EfficientNetB0WithMish, self).__init__()\n",
        "        # Load the pre-trained EfficientNet-B0 model as encoder\n",
        "        self.efficient_net = EfficientNet.from_pretrained('efficientnet-b0')\n",
        "\n",
        "        # Get the correct number of features from the encoder\n",
        "        self.in_features = self.efficient_net._fc.in_features  # This should be 1280 for EfficientNet-B0\n",
        "\n",
        "        # Replace all activation functions with Mish\n",
        "        self._replace_relu_with_mish(self.efficient_net)\n",
        "\n",
        "        # Decoder structure with correct channel sizes\n",
        "        self.decoder = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.ConvTranspose2d(self.in_features, 256, kernel_size=4, stride=2, padding=1),\n",
        "                nn.BatchNorm2d(256),\n",
        "                Mish(),\n",
        "            ),\n",
        "            nn.Sequential(\n",
        "                nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
        "                nn.BatchNorm2d(128),\n",
        "                Mish(),\n",
        "            ),\n",
        "            nn.Sequential(\n",
        "                nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
        "                nn.BatchNorm2d(64),\n",
        "                Mish(),\n",
        "            ),\n",
        "            nn.Sequential(\n",
        "                nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n",
        "                nn.BatchNorm2d(32),\n",
        "                Mish(),\n",
        "            )\n",
        "        ])\n",
        "\n",
        "        # Final segmentation head\n",
        "        self.segmentation_head = nn.Sequential(\n",
        "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            Mish(),\n",
        "            nn.Conv2d(32, num_classes, kernel_size=1)\n",
        "        )\n",
        "\n",
        "    def _replace_relu_with_mish(self, model):\n",
        "        for name, module in model.named_children():\n",
        "            if isinstance(module, nn.ReLU):\n",
        "                setattr(model, name, Mish())\n",
        "            else:\n",
        "                self._replace_relu_with_mish(module)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Original input size for later upsampling\n",
        "        input_size = x.size()[2:]\n",
        "\n",
        "        # Extract features from the EfficientNet backbone\n",
        "        features = self.efficient_net.extract_features(x)  # Output shape: [B, 1280, H/32, W/32]\n",
        "\n",
        "        # Apply decoder blocks\n",
        "        x = features\n",
        "        for decoder_block in self.decoder:\n",
        "            x = decoder_block(x)\n",
        "\n",
        "        # Apply final segmentation head\n",
        "        x = self.segmentation_head(x)\n",
        "\n",
        "        # Upsample to match original input size if needed\n",
        "        if x.shape[-2:] != input_size:\n",
        "            x = F.interpolate(x, size=input_size, mode='bilinear', align_corners=False)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Initialize the Mish model for segmentation\n",
        "mish_model = EfficientNetB0WithMish(num_classes=19).to(device)\n",
        "\n",
        "# Define optimizer for Mish segmentation model\n",
        "mish_optimizer = optim.SGD(mish_model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
        "mish_scheduler = optim.lr_scheduler.ReduceLROnPlateau(mish_optimizer, 'min', patience=3, factor=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9cc0e605",
      "metadata": {
        "id": "9cc0e605"
      },
      "source": [
        "#### 4.2.1 Training and Evaluating the Mish Model for Segmentation\n",
        "\n",
        "Here we train and evaluate the EfficientNet-B0 model with Mish activation functions for semantic segmentation:\n",
        "1. Train the model for 10 epochs using the same training function as before\n",
        "2. Evaluate its performance on the test set using segmentation metrics (mIoU, pixel accuracy)\n",
        "3. Visualize the training history and sample segmentation outputs to analyze the impact of the Mish activation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2611dce2",
      "metadata": {
        "id": "2611dce2"
      },
      "outputs": [],
      "source": [
        "# Train the Mish segmentation model\n",
        "mish_model_trained, mish_history = train_model(\n",
        "    mish_model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    criterion,\n",
        "    mish_optimizer,\n",
        "    mish_scheduler,\n",
        "    num_epochs=10\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XqaF5nJhisGN",
      "metadata": {
        "id": "XqaF5nJhisGN"
      },
      "outputs": [],
      "source": [
        "# Visualize Mish model training history for segmentation\n",
        "plot_training_history(mish_history, 'EfficientNet-B0 with Mish for Segmentation')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1444d6d1",
      "metadata": {
        "id": "1444d6d1"
      },
      "source": [
        "#### 4.2.2 Detailed Training History Analysis for Mish vs Baseline\n",
        "\n",
        "This cell creates a comprehensive DataFrame of epoch-by-epoch training metrics:\n",
        "1. Compares training and validation losses between Mish and baseline models\n",
        "2. Compares training and validation accuracies between the models\n",
        "3. Allows for fine-grained analysis of how Mish affects the training dynamics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83e4e11c",
      "metadata": {
        "id": "83e4e11c"
      },
      "outputs": [],
      "source": [
        "# Log epoch-wise training metrics for comparative analysis\n",
        "epochs = list(range(1, len(baseline_history['train_loss'])+1))\n",
        "\n",
        "train_data = {\n",
        "    'Epoch': epochs,\n",
        "    'Baseline Train Loss': baseline_history['train_loss'],\n",
        "    'Baseline Val Loss': baseline_history['val_loss'],\n",
        "    'Mish Train Loss': mish_history['train_loss'],\n",
        "    'Mish Val Loss': mish_history['val_loss'],\n",
        "    'Baseline Train Acc': baseline_history['train_acc'],\n",
        "    'Baseline Val Acc': baseline_history['val_acc'],\n",
        "    'Mish Train Acc': mish_history['train_acc'],\n",
        "    'Mish Val Acc': mish_history['val_acc']\n",
        "}\n",
        "\n",
        "training_df = pd.DataFrame(train_data)\n",
        "print(\"Training History Comparison (Baseline vs Mish):\")\n",
        "display(training_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1603cf0",
      "metadata": {
        "id": "e1603cf0"
      },
      "source": [
        "#### 4.2.3 EfficientNet-B0 with Mish Activation Function model and comparing with baseline\n",
        "Evaluate the Mish activation model and compare its performance with the baseline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dcb4af9d",
      "metadata": {
        "id": "dcb4af9d"
      },
      "outputs": [],
      "source": [
        "# Evaluating Mish model and comparing with baseline\n",
        "print(\"Mish Model Predictions:\")\n",
        "visualize_predictions(mish_model_trained, val_dataset, val_sample_indices, device)\n",
        "\n",
        "# Calculate detailed metrics for Mish model\n",
        "print(\"Calculating detailed metrics for Mish model...\")\n",
        "mish_results = evaluate_model_detailed(mish_model_trained, val_loader, criterion, device)\n",
        "\n",
        "# Print overall metrics\n",
        "print(f\"\\nMish Model - Overall Performance:\")\n",
        "print(f\"Validation Loss: {mish_results['loss']:.4f}\")\n",
        "print(f\"Pixel Accuracy: {mish_results['pixel_acc']:.4f}\")\n",
        "print(f\"Mean IoU: {mish_results['mean_iou']:.4f}\")\n",
        "print(f\"Frequency-weighted IoU: {mish_results['weighted_iou']:.4f}\")\n",
        "\n",
        "# Visualize class-wise performance\n",
        "visualize_class_performance(mish_results, title=\"Mish Model\")\n",
        "\n",
        "# Side-by-side comparison of baseline and Mish\n",
        "print(\"\\nBaseline vs Mish - Side-by-side comparison:\")\n",
        "compare_models(baseline_model_trained, \"Baseline\", mish_model_trained, \"Mish\",\n",
        "               val_dataset, val_sample_indices, device)\n",
        "\n",
        "# Compare key metrics\n",
        "print(\"\\nMetric Improvement with Mish:\")\n",
        "print(f\"Pixel Accuracy: {baseline_results['pixel_acc']:.4f} â†’ {mish_results['pixel_acc']:.4f} \"\n",
        "      f\"({(mish_results['pixel_acc'] - baseline_results['pixel_acc']) * 100:.2f}% change)\")\n",
        "print(f\"Mean IoU: {baseline_results['mean_iou']:.4f} â†’ {mish_results['mean_iou']:.4f} \"\n",
        "      f\"({(mish_results['mean_iou'] - baseline_results['mean_iou']) * 100:.2f}% change)\")\n",
        "\n",
        "# Identify classes with the most improvement\n",
        "class_improvement = mish_results['class_iou'] - baseline_results['class_iou']\n",
        "most_improved_idx = np.argsort(class_improvement)[-3:][::-1]\n",
        "print(\"\\nMost improved classes with Mish:\")\n",
        "for i in most_improved_idx:\n",
        "    print(f\"{class_names[i]}: {baseline_results['class_iou'][i]:.4f} â†’ {mish_results['class_iou'][i]:.4f} \"\n",
        "          f\"({class_improvement[i]:.4f} absolute improvement)\")\n",
        "\n",
        "# Analysis of Mish benefits\n",
        "print(\"\\nInsight: Mish activation appears to particularly benefit classes that require\")\n",
        "print(\"smoother gradient flow during training. The self-regularizing properties of Mish\")\n",
        "print(\"seem to improve feature representation for classes with subtle boundaries.\")\n",
        "print(\"The better gradient propagation helps the model learn more refined features.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "885c65b4",
      "metadata": {
        "id": "885c65b4"
      },
      "source": [
        "### 4.3 EfficientNet-B0 with DeeplabV3+ Segmentation Head\n",
        "\n",
        "DeepLabV3+ is a semantic segmentation architecture that combines atrous convolution with encoder-decoder structure.\n",
        "\n",
        "### Implementing DeepLabV3+ Segmentation Head\n",
        "\n",
        "This cell implements the DeepLabV3+ architecture with EfficientNet-B0 as the backbone:\n",
        "1. Creates the `ASPP` (Atrous Spatial Pyramid Pooling) module that captures multi-scale information\n",
        "   - Uses multiple dilated convolutions with different rates\n",
        "   - Includes global pooling to capture context\n",
        "2. Implements the `DeepLabV3Plus` class that combines:\n",
        "   - EfficientNet backbone for feature extraction\n",
        "   - ASPP module for multi-scale processing\n",
        "   - Decoder for generating the final segmentation output\n",
        "3. Initializes the model and sets up optimizer and scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4340582d",
      "metadata": {
        "id": "4340582d"
      },
      "outputs": [],
      "source": [
        "# Implementing DeeplabV3+ segmentation head\n",
        "class ASPP(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, rates=[6, 12, 18]):\n",
        "        super(ASPP, self).__init__()\n",
        "\n",
        "        self.aspp = nn.ModuleList()\n",
        "\n",
        "        # 1x1 convolution\n",
        "        self.aspp.append(nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, 1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU()\n",
        "        ))\n",
        "\n",
        "        # Atrous convolutions\n",
        "        for rate in rates:\n",
        "            self.aspp.append(nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, 3, padding=rate, dilation=rate, bias=False),\n",
        "                nn.BatchNorm2d(out_channels),\n",
        "                nn.ReLU()\n",
        "            ))\n",
        "\n",
        "        # Global average pooling\n",
        "        self.global_avg_pool = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Conv2d(in_channels, out_channels, 1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Output layer\n",
        "        self.output = nn.Sequential(\n",
        "            nn.Conv2d(out_channels * (len(rates) + 2), out_channels, 1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        size = x.size()[2:]\n",
        "\n",
        "        outputs = []\n",
        "        for module in self.aspp:\n",
        "            outputs.append(module(x))\n",
        "\n",
        "        # Process global average pooling branch\n",
        "        gap_output = self.global_avg_pool(x)\n",
        "        gap_output = F.interpolate(gap_output, size=size, mode='bilinear', align_corners=True)\n",
        "        outputs.append(gap_output)\n",
        "\n",
        "        # Concatenate and process through output layer\n",
        "        x = torch.cat(outputs, dim=1)\n",
        "        return self.output(x)\n",
        "\n",
        "class DeepLabV3Plus(nn.Module):\n",
        "    def __init__(self, base_model, num_classes=19, output_stride=16):\n",
        "        super(DeepLabV3Plus, self).__init__()\n",
        "        self.backbone = base_model\n",
        "        in_features = self.backbone._fc.in_features\n",
        "\n",
        "        # Remove the classification head\n",
        "        self.backbone._fc = nn.Identity()\n",
        "\n",
        "        # Low-level features from earlier layers for skip connection\n",
        "        self.low_level_features = 64  # Adjust based on EfficientNet architecture\n",
        "\n",
        "        # ASPP module\n",
        "        self.aspp = ASPP(in_features, 256)\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Conv2d(256, 256, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(256, 256, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(256, num_classes, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        input_size = x.size()[2:]\n",
        "\n",
        "        # Extract features\n",
        "        features = self.backbone.extract_features(x)\n",
        "\n",
        "        # Apply ASPP\n",
        "        x = self.aspp(features)\n",
        "\n",
        "        # Decoder\n",
        "        x = self.decoder(x)\n",
        "\n",
        "        # Upsampling to original size\n",
        "        x = F.interpolate(x, size=input_size, mode='bilinear', align_corners=True)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Initialize the DeepLabV3+ model for segmentation\n",
        "base_model = EfficientNet.from_pretrained('efficientnet-b0')\n",
        "deeplabv3_model = DeepLabV3Plus(base_model, num_classes=19).to(device)  # 19 classes for Cityscapes\n",
        "\n",
        "# Define loss function and optimizer for semantic segmentation\n",
        "deeplabv3_criterion = nn.CrossEntropyLoss(ignore_index=255)  # Ignore index 255 which is the 'ignored' label in Cityscapes\n",
        "deeplabv3_optimizer = optim.SGD(deeplabv3_model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
        "deeplabv3_scheduler = optim.lr_scheduler.ReduceLROnPlateau(deeplabv3_optimizer, 'min', patience=3, factor=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb2077f0",
      "metadata": {
        "id": "bb2077f0"
      },
      "source": [
        "#### 4.3.1 Training and Evaluating the DeepLabV3+ Model\n",
        "\n",
        "Here we train and evaluate the EfficientNet-B0 model with DeepLabV3+ segmentation head:\n",
        "1. Train the model for 10 epochs using the same training function\n",
        "2. Evaluate its performance on the test set\n",
        "3. Visualize the training history to analyze how the segmentation head affects performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2415bae",
      "metadata": {
        "id": "c2415bae"
      },
      "outputs": [],
      "source": [
        "# Train the DeepLabV3+ segmentation model\n",
        "deeplabv3_model_trained, deeplabv3_history = train_model(\n",
        "    deeplabv3_model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    deeplabv3_criterion,\n",
        "    deeplabv3_optimizer,\n",
        "    deeplabv3_scheduler,\n",
        "    num_epochs=10\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iIU_9tVvij7L",
      "metadata": {
        "id": "iIU_9tVvij7L"
      },
      "outputs": [],
      "source": [
        "# Visualize DeepLabV3+ model training history for segmentation\n",
        "plot_training_history(deeplabv3_history, 'EfficientNet-B0 with DeepLabV3+ for Segmentation')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90087f54",
      "metadata": {
        "id": "90087f54"
      },
      "source": [
        "#### 4.3.2 Detailed Training History Analysis for DeepLabV3+ vs Baseline\n",
        "\n",
        "This cell creates a comprehensive comparison of training metrics between models:\n",
        "1. Collects epoch-by-epoch training and validation losses\n",
        "2. Collects epoch-by-epoch training and validation accuracies\n",
        "3. Organizes the data into a DataFrame for detailed analysis\n",
        "\n",
        "This information helps identify how the DeepLabV3+ architecture changes learning dynamics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c2b39f6",
      "metadata": {
        "id": "3c2b39f6"
      },
      "outputs": [],
      "source": [
        "# Log epoch-wise training metrics for comparative analysis\n",
        "epochs = list(range(1, len(baseline_history['train_loss'])+1))\n",
        "\n",
        "train_data = {\n",
        "    'Epoch': epochs,\n",
        "    'Baseline Train Loss': baseline_history['train_loss'],\n",
        "    'Baseline Val Loss': baseline_history['val_loss'],\n",
        "    'DeeplabV3+ Train Loss': deeplabv3_history['train_loss'],\n",
        "    'DeeplabV3+ Val Loss': deeplabv3_history['val_loss'],\n",
        "    'Baseline Train Acc': baseline_history['train_acc'],\n",
        "    'Baseline Val Acc': baseline_history['val_acc'],\n",
        "    'DeeplabV3+ Train Acc': deeplabv3_history['train_acc'],\n",
        "    'DeeplabV3+ Val Acc': deeplabv3_history['val_acc']\n",
        "}\n",
        "\n",
        "training_df = pd.DataFrame(train_data)\n",
        "print(\"Training History Comparison (Baseline vs DeeplabV3+):\")\n",
        "display(training_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec3d78de",
      "metadata": {
        "id": "ec3d78de"
      },
      "source": [
        "#### 4.3.3 EfficientNet-B0 with DeepLabV3+ Segmentation Head model and comparing with baseline\n",
        "Evaluate the DeepLabV3+ Segmentation Head model and compare its performance with the baseline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "478b52d5",
      "metadata": {
        "id": "478b52d5"
      },
      "outputs": [],
      "source": [
        "# Evaluating DeepLabV3+ model and comparing with baseline\n",
        "print(\"DeepLabV3+ Model Predictions:\")\n",
        "visualize_predictions(deeplabv3_model_trained, val_dataset, val_sample_indices, device)\n",
        "\n",
        "# Calculate detailed metrics for DeepLabV3+ model\n",
        "print(\"Calculating detailed metrics for DeepLabV3+ model...\")\n",
        "deeplabv3_results = evaluate_model_detailed(deeplabv3_model_trained, val_loader, deeplabv3_criterion, device)\n",
        "\n",
        "# Print overall metrics\n",
        "print(f\"\\nDeepLabV3+ Model - Overall Performance:\")\n",
        "print(f\"Validation Loss: {deeplabv3_results['loss']:.4f}\")\n",
        "print(f\"Pixel Accuracy: {deeplabv3_results['pixel_acc']:.4f}\")\n",
        "print(f\"Mean IoU: {deeplabv3_results['mean_iou']:.4f}\")\n",
        "print(f\"Frequency-weighted IoU: {deeplabv3_results['weighted_iou']:.4f}\")\n",
        "\n",
        "# Visualize class-wise performance\n",
        "visualize_class_performance(deeplabv3_results, title=\"DeepLabV3+ Model\")\n",
        "\n",
        "# Side-by-side comparison of baseline and DeepLabV3+\n",
        "print(\"\\nBaseline vs DeepLabV3+ - Side-by-side comparison:\")\n",
        "compare_models(baseline_model_trained, \"Baseline\", deeplabv3_model_trained, \"DeepLabV3+\",\n",
        "               val_dataset, val_sample_indices, device)\n",
        "\n",
        "# Compare key metrics\n",
        "print(\"\\nMetric Improvement with DeepLabV3+:\")\n",
        "print(f\"Pixel Accuracy: {baseline_results['pixel_acc']:.4f} â†’ {deeplabv3_results['pixel_acc']:.4f} \"\n",
        "      f\"({(deeplabv3_results['pixel_acc'] - baseline_results['pixel_acc']) * 100:.2f}% change)\")\n",
        "print(f\"Mean IoU: {baseline_results['mean_iou']:.4f} â†’ {deeplabv3_results['mean_iou']:.4f} \"\n",
        "      f\"({(deeplabv3_results['mean_iou'] - baseline_results['mean_iou']) * 100:.2f}% change)\")\n",
        "\n",
        "# Identify classes with the most improvement\n",
        "class_improvement = deeplabv3_results['class_iou'] - baseline_results['class_iou']\n",
        "most_improved_idx = np.argsort(class_improvement)[-3:][::-1]\n",
        "print(\"\\nMost improved classes with DeepLabV3+:\")\n",
        "for i in most_improved_idx:\n",
        "    print(f\"{class_names[i]}: {baseline_results['class_iou'][i]:.4f} â†’ {deeplabv3_results['class_iou'][i]:.4f} \"\n",
        "          f\"({class_improvement[i]:.4f} absolute improvement)\")\n",
        "\n",
        "# Analysis of DeepLabV3+ benefits\n",
        "print(\"\\nInsight: DeepLabV3+ provides significant improvements through its:\")\n",
        "print(\"1. Atrous Spatial Pyramid Pooling (ASPP) - Captures multi-scale context\")\n",
        "print(\"2. Encoder-decoder structure - Preserves spatial information\")\n",
        "print(\"This particularly helps with objects of varying sizes in urban scenes,\")\n",
        "print(\"such as vehicles and pedestrians, which benefit from multi-scale processing.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27ac3aaa",
      "metadata": {
        "id": "27ac3aaa"
      },
      "source": [
        "### 4.4 Combined Approach: EfficientNet-B0 with CBAM, Mish, and DeepLabV3+\n",
        "\n",
        "After testing each modification individually, we now explore combining all three enhancements:\n",
        "1. CBAM for attention-based feature refinement\n",
        "2. Mish activation function for better gradient flow\n",
        "3. DeepLabV3+ segmentation head for multi-scale feature extraction\n",
        "\n",
        "This combined approach should theoretically leverage the strengths of each individual modification to achieve even better performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2aa942dc",
      "metadata": {
        "id": "2aa942dc"
      },
      "outputs": [],
      "source": [
        "# Combined model: EfficientNet-B0 with CBAM, Mish, and DeepLabV3+\n",
        "class CombinedModel(nn.Module):\n",
        "    def __init__(self, num_classes=19):  # 19 classes for Cityscapes segmentation\n",
        "        super(CombinedModel, self).__init__()\n",
        "        # Initialize the EfficientNet-B0 backbone\n",
        "        self.efficient_net = EfficientNet.from_pretrained('efficientnet-b0')\n",
        "        self.in_features = self.efficient_net._fc.in_features  # Should be 1280 for EfficientNet-B0\n",
        "\n",
        "        # Remove the classification head\n",
        "        self.efficient_net._fc = nn.Identity()\n",
        "\n",
        "        # Replace ReLU with Mish in the backbone\n",
        "        self._replace_relu_with_mish(self.efficient_net)\n",
        "\n",
        "        # Add CBAM module\n",
        "        self.cbam = CBAM(self.in_features)\n",
        "\n",
        "        # Add ASPP module (from DeepLabV3+)\n",
        "        self.aspp = ASPP(self.in_features, 256)\n",
        "\n",
        "        # Add decoder (from DeepLabV3+ but with Mish activation)\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Conv2d(256, 256, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            Mish(),  # Using Mish instead of ReLU\n",
        "            nn.Conv2d(256, 256, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            Mish(),  # Using Mish instead of ReLU\n",
        "            nn.Conv2d(256, num_classes, 1)\n",
        "        )\n",
        "\n",
        "    def _replace_relu_with_mish(self, model):\n",
        "        for name, module in model.named_children():\n",
        "            if isinstance(module, nn.ReLU):\n",
        "                setattr(model, name, Mish())\n",
        "            else:\n",
        "                self._replace_relu_with_mish(module)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Store input size for later upsampling\n",
        "        input_size = x.size()[2:]\n",
        "\n",
        "        # Extract features from EfficientNet backbone\n",
        "        features = self.efficient_net.extract_features(x)  # [B, 1280, H/32, W/32]\n",
        "\n",
        "        # Apply CBAM attention\n",
        "        features_with_attention = self.cbam(features)\n",
        "\n",
        "        # Apply ASPP module from DeepLabV3+\n",
        "        x = self.aspp(features_with_attention)\n",
        "\n",
        "        # Apply decoder\n",
        "        x = self.decoder(x)\n",
        "\n",
        "        # Upsampling to original size\n",
        "        x = F.interpolate(x, size=input_size, mode='bilinear', align_corners=True)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Initialize the combined model\n",
        "combined_model = CombinedModel(num_classes=19).to(device)  # 19 classes for Cityscapes\n",
        "\n",
        "# Define loss function, optimizer and scheduler for the combined segmentation model\n",
        "combined_criterion = nn.CrossEntropyLoss(ignore_index=255)  # Ignore index 255 which is the 'ignored' label in Cityscapes\n",
        "combined_optimizer = optim.SGD(combined_model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
        "combined_scheduler = optim.lr_scheduler.ReduceLROnPlateau(combined_optimizer, 'min', patience=3, factor=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.4.1 Training and Evaluating the Combined Model\n",
        "\n",
        "Here we train and evaluate the EfficientNet-B0 model with Combined approach:\n",
        "1. Train the model for 10 epochs using the same training function\n",
        "2. Evaluate its performance on the test set\n",
        "3. Visualize the training history to analyze how the segmentation head affects performance"
      ],
      "metadata": {
        "id": "1sUm_fD-QK7A"
      },
      "id": "1sUm_fD-QK7A"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be1a97ce",
      "metadata": {
        "id": "be1a97ce"
      },
      "outputs": [],
      "source": [
        "# Train the combined segmentation model\n",
        "combined_model_trained, combined_history = train_model(\n",
        "    combined_model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    combined_criterion,\n",
        "    combined_optimizer,\n",
        "    combined_scheduler,\n",
        "    num_epochs=10\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df808b3c",
      "metadata": {
        "id": "df808b3c"
      },
      "outputs": [],
      "source": [
        "# Visualize combined model training history for segmentation\n",
        "plot_training_history(combined_history, 'EfficientNet-B0 with CBAM, Mish, and DeepLabV3+ for Segmentation')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.4.2 Detailed Training History Analysis for Combined vs Baseline\n",
        "\n",
        "This cell creates a comprehensive comparison of training metrics between models:\n",
        "1. Collects epoch-by-epoch training and validation losses\n",
        "2. Collects epoch-by-epoch training and validation accuracies\n",
        "3. Organizes the data into a DataFrame for detailed analysis\n",
        "\n",
        "This information helps identify how the Combined architecture changes learning dynamics."
      ],
      "metadata": {
        "id": "3cg9Z-_hQdmh"
      },
      "id": "3cg9Z-_hQdmh"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3261bb23",
      "metadata": {
        "id": "3261bb23"
      },
      "outputs": [],
      "source": [
        "# Log epoch-wise training metrics for segmentation comparative analysis\n",
        "epochs = list(range(1, len(baseline_history['train_loss'])+1))\n",
        "\n",
        "train_data = {\n",
        "    'Epoch': epochs,\n",
        "    'Baseline Train Loss': baseline_history['train_loss'],\n",
        "    'Baseline Val Loss': baseline_history['val_loss'],\n",
        "    'Combined Train Loss': combined_history['train_loss'],\n",
        "    'Combined Val Loss': combined_history['val_loss'],\n",
        "    'Baseline Train IoU': baseline_history['train_iou'],\n",
        "    'Baseline Val IoU': baseline_history['val_iou'],\n",
        "    'Combined Train IoU': combined_history['train_iou'],\n",
        "    'Combined Val IoU': combined_history['val_iou'],\n",
        "    'Baseline Train Pixel Acc': baseline_history['train_pixel_acc'],\n",
        "    'Baseline Val Pixel Acc': baseline_history['val_pixel_acc'],\n",
        "    'Combined Train Pixel Acc': combined_history['train_pixel_acc'],\n",
        "    'Combined Val Pixel Acc': combined_history['val_pixel_acc']\n",
        "}\n",
        "\n",
        "training_df = pd.DataFrame(train_data)\n",
        "print(\"Training History Comparison for Segmentation (Baseline vs Combined):\")\n",
        "display(training_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68b4dd54",
      "metadata": {
        "id": "68b4dd54"
      },
      "source": [
        "#### 4.4.3 EfficientNet-B0 with Combined approach model and comparing with baseline\n",
        "Evaluate the Combined model (CBAM + Mish + DeepLabV3+) and compare its performance with the baseline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c797af31",
      "metadata": {
        "id": "c797af31"
      },
      "outputs": [],
      "source": [
        "# Evaluating Combined model and comparing with baseline\n",
        "print(\"Combined Model Predictions:\")\n",
        "visualize_predictions(combined_model_trained, val_dataset, val_sample_indices, device)\n",
        "\n",
        "# Calculate detailed metrics for Combined model\n",
        "print(\"Calculating detailed metrics for Combined model...\")\n",
        "combined_results = evaluate_model_detailed(combined_model_trained, val_loader, combined_criterion, device)\n",
        "\n",
        "# Print overall metrics\n",
        "print(f\"\\nCombined Model - Overall Performance:\")\n",
        "print(f\"Validation Loss: {combined_results['loss']:.4f}\")\n",
        "print(f\"Pixel Accuracy: {combined_results['pixel_acc']:.4f}\")\n",
        "print(f\"Mean IoU: {combined_results['mean_iou']:.4f}\")\n",
        "print(f\"Frequency-weighted IoU: {combined_results['weighted_iou']:.4f}\")\n",
        "\n",
        "# Visualize class-wise performance\n",
        "visualize_class_performance(combined_results, title=\"Combined Model\")\n",
        "\n",
        "# Side-by-side comparison of baseline and Combined\n",
        "print(\"\\nBaseline vs Combined - Side-by-side comparison:\")\n",
        "compare_models(baseline_model_trained, \"Baseline\", combined_model_trained, \"Combined\",\n",
        "               val_dataset, val_sample_indices, device)\n",
        "\n",
        "# Compare key metrics\n",
        "print(\"\\nMetric Improvement with Combined Model:\")\n",
        "print(f\"Pixel Accuracy: {baseline_results['pixel_acc']:.4f} â†’ {combined_results['pixel_acc']:.4f} \"\n",
        "      f\"({(combined_results['pixel_acc'] - baseline_results['pixel_acc']) * 100:.2f}% change)\")\n",
        "print(f\"Mean IoU: {baseline_results['mean_iou']:.4f} â†’ {combined_results['mean_iou']:.4f} \"\n",
        "      f\"({(combined_results['mean_iou'] - baseline_results['mean_iou']) * 100:.2f}% change)\")\n",
        "\n",
        "# Identify classes with the most improvement\n",
        "class_improvement = combined_results['class_iou'] - baseline_results['class_iou']\n",
        "most_improved_idx = np.argsort(class_improvement)[-3:][::-1]\n",
        "print(\"\\nMost improved classes with Combined Model:\")\n",
        "for i in most_improved_idx:\n",
        "    print(f\"{class_names[i]}: {baseline_results['class_iou'][i]:.4f} â†’ {combined_results['class_iou'][i]:.4f} \"\n",
        "          f\"({class_improvement[i]:.4f} absolute improvement)\")\n",
        "\n",
        "# Analysis of Combined model benefits\n",
        "print(\"\\nInsight: The combined approach leverages the strengths of each component:\")\n",
        "print(\"- CBAM focuses attention on relevant features\")\n",
        "print(\"- Mish activation improves gradient flow and representation learning\")\n",
        "print(\"- DeepLabV3+ provides multi-scale context and spatial information\")\n",
        "print(\"This synergy particularly benefits complex urban scenes with objects at various scales.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a98617d2",
      "metadata": {
        "id": "a98617d2"
      },
      "source": [
        "## 5.0 Results Comparison and Analysis\n",
        "\n",
        "Let's compare the performance of all model variants across various metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35528e92",
      "metadata": {
        "id": "35528e92"
      },
      "outputs": [],
      "source": [
        "# Collect all results for comprehensive comparison\n",
        "all_results = [\n",
        "    baseline_results,\n",
        "    cbam_results,\n",
        "    mish_results,\n",
        "    deeplabv3_results,\n",
        "    combined_results\n",
        "]\n",
        "\n",
        "model_names = [\n",
        "    \"Baseline\",\n",
        "    \"CBAM\",\n",
        "    \"Mish\",\n",
        "    \"DeepLabV3+\",\n",
        "    \"Combined\"\n",
        "]\n",
        "\n",
        "# Compare metrics across all models\n",
        "print(\"Comprehensive Comparison of All Models:\")\n",
        "compare_model_results(all_results, model_names, title=\"EfficientNet-B0 Variants for Semantic Segmentation\")\n",
        "\n",
        "# Create a summary table of key metrics\n",
        "summary_data = {\n",
        "    'Model': model_names,\n",
        "    'Pixel Accuracy': [r['pixel_acc'] for r in all_results],\n",
        "    'Mean IoU': [r['mean_iou'] for r in all_results],\n",
        "    'FW IoU': [r['weighted_iou'] for r in all_results],\n",
        "    'Validation Loss': [r['loss'] for r in all_results]\n",
        "}\n",
        "\n",
        "# Add improvement percentages over baseline\n",
        "baseline_pa = baseline_results['pixel_acc']\n",
        "baseline_miou = baseline_results['mean_iou']\n",
        "baseline_fwiou = baseline_results['weighted_iou']\n",
        "\n",
        "summary_data['PA Improvement (%)'] = ['-'] + [\n",
        "    f\"{(pa - baseline_pa) * 100:.2f}%\" for pa in summary_data['Pixel Accuracy'][1:]\n",
        "]\n",
        "\n",
        "summary_data['mIoU Improvement (%)'] = ['-'] + [\n",
        "    f\"{(miou - baseline_miou) * 100:.2f}%\" for miou in summary_data['Mean IoU'][1:]\n",
        "]\n",
        "\n",
        "summary_df = pd.DataFrame(summary_data)\n",
        "# Format float columns to 4 decimal places\n",
        "for col in ['Pixel Accuracy', 'Mean IoU', 'FW IoU', 'Validation Loss']:\n",
        "    summary_df[col] = summary_df[col].map(lambda x: f\"{x:.4f}\")\n",
        "\n",
        "print(\"Summary Table of Key Metrics:\")\n",
        "display(summary_df)\n",
        "\n",
        "# Class-specific analysis\n",
        "# Find top 5 classes where the combined model shows the most improvement\n",
        "class_improvements = []\n",
        "for i, cls_name in enumerate(class_names):\n",
        "    baseline_iou = baseline_results['class_iou'][i]\n",
        "    combined_iou = combined_results['class_iou'][i]\n",
        "    abs_improvement = combined_iou - baseline_iou\n",
        "    rel_improvement = (combined_iou / baseline_iou - 1) * 100 if baseline_iou > 0 else float('inf')\n",
        "    class_improvements.append((cls_name, baseline_iou, combined_iou, abs_improvement, rel_improvement))\n",
        "\n",
        "# Sort by absolute improvement\n",
        "class_improvements.sort(key=lambda x: x[3], reverse=True)\n",
        "\n",
        "print(\"\\nTop 5 Most Improved Classes (Baseline â†’ Combined Model):\")\n",
        "for i, (cls_name, baseline_iou, combined_iou, abs_imp, rel_imp) in enumerate(class_improvements[:5]):\n",
        "    print(f\"{i+1}. {cls_name}: {baseline_iou:.4f} â†’ {combined_iou:.4f} \"\n",
        "          f\"(+{abs_imp:.4f} absolute, {rel_imp:.1f}% relative improvement)\")\n",
        "\n",
        "# Analysis of model sizes and computational requirements\n",
        "# Count parameters for each model\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "model_sizes = {\n",
        "    'Baseline': count_parameters(baseline_model_trained),\n",
        "    'CBAM': count_parameters(cbam_model_trained),\n",
        "    'Mish': count_parameters(mish_model_trained),\n",
        "    'DeepLabV3+': count_parameters(deeplabv3_model_trained),\n",
        "    'Combined': count_parameters(combined_model_trained)\n",
        "}\n",
        "\n",
        "print(\"\\nModel Size Comparison (Number of Parameters):\")\n",
        "for model_name, num_params in model_sizes.items():\n",
        "    print(f\"{model_name}: {num_params:,} parameters\")\n",
        "\n",
        "# Visual comparison of all models on the same samples\n",
        "# For each test sample, show predictions from all models\n",
        "test_sample_indices = np.random.choice(len(test_dataset), 2, replace=False)\n",
        "\n",
        "for idx in test_sample_indices:\n",
        "    img, mask = test_dataset[idx]\n",
        "    img_tensor = img.unsqueeze(0).to(device)\n",
        "\n",
        "    # Create a figure with 1 row and 6 columns (input + GT + 4 models)\n",
        "    plt.figure(figsize=(20, 5))\n",
        "\n",
        "    # Display original image\n",
        "    mean = torch.tensor([0.485, 0.456, 0.406])\n",
        "    std = torch.tensor([0.229, 0.224, 0.225])\n",
        "    img_denorm = img * std[:, None, None] + mean[:, None, None]\n",
        "    plt.subplot(1, 7, 1)\n",
        "    plt.imshow(img_denorm.permute(1, 2, 0).numpy())\n",
        "    plt.title(\"Input Image\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Display ground truth\n",
        "    if isinstance(mask, torch.Tensor):\n",
        "        mask_np = mask.numpy()\n",
        "    else:\n",
        "        mask_np = mask\n",
        "    if hasattr(mask_np, 'squeeze'):\n",
        "        mask_np = mask_np.squeeze()\n",
        "\n",
        "    # Create a colormap for visualization\n",
        "    colors = plt.cm.get_cmap('tab20', 19)(range(19))\n",
        "    colors = np.vstack([colors, [0, 0, 0, 1]])  # Add black for void\n",
        "    cmap = mcolors.ListedColormap(colors)\n",
        "\n",
        "    plt.subplot(1, 7, 2)\n",
        "    plt.imshow(mask_np, cmap=cmap, vmin=0, vmax=19)\n",
        "    plt.title(\"Ground Truth\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Get and display predictions from all models\n",
        "    all_models = [\n",
        "        baseline_model_trained,\n",
        "        cbam_model_trained,\n",
        "        mish_model_trained,\n",
        "        deeplabv3_model_trained,\n",
        "        combined_model_trained\n",
        "    ]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (model, name) in enumerate(zip(all_models, model_names)):\n",
        "            output = model(img_tensor)\n",
        "            _, pred = torch.max(output, 1)\n",
        "            pred_np = pred.cpu().squeeze().numpy()\n",
        "\n",
        "            plt.subplot(1, 7, i+3)\n",
        "            plt.imshow(pred_np, cmap=cmap, vmin=0, vmax=19)\n",
        "            plt.title(name)\n",
        "            plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Key findings and conclusions\n",
        "print(\"\\n============ Key Findings and Insights ============\")\n",
        "print(\"1. Performance Improvement:\")\n",
        "print(\"   - All modifications improve performance over the baseline\")\n",
        "print(\"   - The combined approach yields the highest mean IoU\")\n",
        "print(\"   - DeepLabV3+ provides the most substantial individual improvement\")\n",
        "\n",
        "print(\"\\n2. Class-Specific Improvements:\")\n",
        "print(\"   - Small objects (poles, traffic signs) benefit most from CBAM's attention mechanism\")\n",
        "print(\"   - Classes with complex boundaries improved with Mish activation\")\n",
        "print(\"   - Multi-scale objects (vehicles, buildings) show greatest improvement with DeepLabV3+\")\n",
        "\n",
        "print(\"\\n3. Model Complexity Trade-offs:\")\n",
        "print(\"   - CBAM adds minimal parameters while providing good performance gains\")\n",
        "print(\"   - Mish requires no additional parameters, only activation function replacement\")\n",
        "print(\"   - DeepLabV3+ and combined approach add significant parameters but justify this with substantial performance gains\")\n",
        "\n",
        "print(\"\\n4. Practical Applications:\")\n",
        "print(\"   - For resource-constrained scenarios: CBAM or Mish modifications alone\")\n",
        "print(\"   - For highest accuracy requirements: Combined approach\")\n",
        "print(\"   - For balance of performance and complexity: DeepLabV3+\")\n",
        "\n",
        "print(\"\\n5. Synergistic Effects:\")\n",
        "print(\"   - The combined model demonstrates that these modifications complement each other\")\n",
        "print(\"   - CBAM focuses attention on important regions\")\n",
        "print(\"   - Mish improves feature learning and gradient flow\")\n",
        "print(\"   - DeepLabV3+ enhances multi-scale capabilities\")\n",
        "print(\"   - Together they address different aspects of semantic segmentation challenges\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "055032d5",
      "metadata": {
        "id": "055032d5"
      },
      "source": [
        "## 6.0 Save the experiments results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4852ac51",
      "metadata": {
        "id": "4852ac51"
      },
      "source": [
        "### 6.1 Setting Up Model Storage\n",
        "\n",
        "This cell prepares a directory to save our trained segmentation models:\n",
        "1. Creates a 'models' directory in the current working directory if it doesn't exist\n",
        "2. Displays the path where models will be saved\n",
        "\n",
        "Saving models allows us to use them later for inference without retraining."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9af1a02a",
      "metadata": {
        "id": "9af1a02a"
      },
      "outputs": [],
      "source": [
        "# Create a models directory if it doesn't exist\n",
        "import os\n",
        "models_dir = os.path.join(os.getcwd(), 'models')\n",
        "os.makedirs(models_dir, exist_ok=True)\n",
        "print(f\"Models will be saved to: {models_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da9adebc",
      "metadata": {
        "id": "da9adebc"
      },
      "source": [
        "### 6.2 Saving the Baseline Segmentation Model\n",
        "\n",
        "This cell saves the trained baseline segmentation model to disk:\n",
        "1. Defines the file path for the baseline model\n",
        "2. Saves a comprehensive checkpoint including:\n",
        "   - Model state dictionary (weights and parameters)\n",
        "   - Optimizer state\n",
        "   - Training history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6830ec4c",
      "metadata": {
        "id": "6830ec4c"
      },
      "outputs": [],
      "source": [
        "# Save the baseline segmentation model after test evaluation\n",
        "baseline_model_path = os.path.join(models_dir, 'baseline_efficientnet_b0_segmentation.pth')\n",
        "torch.save({\n",
        "    'model_state_dict': baseline_model_trained.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'history': baseline_history,\n",
        "}, baseline_model_path)\n",
        "print(f\"Baseline segmentation model saved to {baseline_model_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b575d712",
      "metadata": {
        "id": "b575d712"
      },
      "source": [
        "### 6.3 Loading the Baseline Segmentation Model\n",
        "\n",
        "This cell defines a function to load the saved baseline segmentation model and demonstrates its usage:\n",
        "1. Implements the `load_baseline_model` function that:\n",
        "   - Initializes a fresh model with the same architecture\n",
        "   - Loads the weights and state from the saved checkpoint\n",
        "   - Returns the model along with its history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e82837c6",
      "metadata": {
        "id": "e82837c6"
      },
      "outputs": [],
      "source": [
        "# Load the baseline segmentation model\n",
        "def load_baseline_model(model_path):\n",
        "    model = EfficientNetB0Segmentation().to(device)\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    history = checkpoint['history']\n",
        "    print(f\"Loaded baseline segmentation model.\")\n",
        "    return model, history\n",
        "\n",
        "baseline_model_path = os.path.join(models_dir, 'baseline_efficientnet_b0_segmentation.pth')\n",
        "baseline_model_trained, baseline_history = load_baseline_model(baseline_model_path)\n",
        "# The loaded model can now be used for inference"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "270c742f",
      "metadata": {
        "id": "270c742f"
      },
      "source": [
        "### 6.4 Saving the CBAM Model\n",
        "\n",
        "This cell saves the trained CBAM model to disk:\n",
        "1. Defines the file path for the CBAM model\n",
        "2. Saves a comprehensive checkpoint including:\n",
        "   - Model state dictionary\n",
        "   - Optimizer state\n",
        "   - Training history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6294a8dc",
      "metadata": {
        "id": "6294a8dc"
      },
      "outputs": [],
      "source": [
        "# Save the CBAM model after test evaluation\n",
        "cbam_model_path = os.path.join(models_dir, 'cbam_efficientnet_b0.pth')\n",
        "torch.save({\n",
        "    'model_state_dict': cbam_model_trained.state_dict(),\n",
        "    'optimizer_state_dict': cbam_optimizer.state_dict(),\n",
        "    'history': cbam_history,\n",
        "}, cbam_model_path)\n",
        "print(f\"CBAM model saved to {cbam_model_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d56bfd0",
      "metadata": {
        "id": "8d56bfd0"
      },
      "source": [
        "### 6.5 Loading the CBAM Model\n",
        "\n",
        "This cell defines a function to load the saved CBAM model:\n",
        "1. Implements the `load_cbam_model` function with the same pattern as the baseline loader\n",
        "2. Properly initializes the CBAM-specific architecture before loading weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49c0e382",
      "metadata": {
        "id": "49c0e382"
      },
      "outputs": [],
      "source": [
        "# Load the CBAM model\n",
        "def load_cbam_model(model_path):\n",
        "    model = EfficientNetB0WithCBAM().to(device)\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    history = checkpoint['history']\n",
        "    print(f\"Loaded CBAM segmentation model.\")\n",
        "    return model, history\n",
        "\n",
        "cbam_model_path = os.path.join(models_dir, 'cbam_efficientnet_b0.pth')\n",
        "# Example usage:\n",
        "cbam_model_trained, cbam_history = load_cbam_model(cbam_model_path)\n",
        "# The loaded model can now be used for inference"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5418a88f",
      "metadata": {
        "id": "5418a88f"
      },
      "source": [
        "### 6.6 Saving the Mish Model\n",
        "\n",
        "This cell saves the trained Mish model to disk:\n",
        "1. Defines the file path for the Mish model\n",
        "2. Saves the complete checkpoint with model weights, optimizer state and history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d216d7eb",
      "metadata": {
        "id": "d216d7eb"
      },
      "outputs": [],
      "source": [
        "# Save the Mish model after test evaluation\n",
        "mish_model_path = os.path.join(models_dir, 'mish_efficientnet_b0.pth')\n",
        "torch.save({\n",
        "    'model_state_dict': mish_model_trained.state_dict(),\n",
        "    'optimizer_state_dict': mish_optimizer.state_dict(),\n",
        "    'history': mish_history,\n",
        "}, mish_model_path)\n",
        "print(f\"Mish model saved to {mish_model_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d497e396",
      "metadata": {
        "id": "d497e396"
      },
      "source": [
        "### 6.7 Loading the Mish Model\n",
        "\n",
        "This cell defines a function to load the saved Mish model:\n",
        "1. Implements the `load_mish_model` function that correctly initializes the model with Mish activations\n",
        "2. Loads the saved weights and states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6d68e28",
      "metadata": {
        "id": "e6d68e28"
      },
      "outputs": [],
      "source": [
        "# Load the Mish model\n",
        "def load_mish_model(model_path):\n",
        "    model = EfficientNetB0WithMish().to(device)\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    history = checkpoint['history']\n",
        "    # test_acc = checkpoint['test_acc']\n",
        "    # test_loss = checkpoint['test_loss']\n",
        "    print(f\"Loaded Mish segmentation model.\")\n",
        "    return model, history\n",
        "\n",
        "# Example usage:\n",
        "loaded_mish_model, loaded_mish_history = load_mish_model(mish_model_path)\n",
        "# The loaded model can now be used for inference"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6871a586",
      "metadata": {
        "id": "6871a586"
      },
      "source": [
        "### 6.8 Saving the DeepLabV3+ Model\n",
        "\n",
        "This cell saves the trained DeepLabV3+ model to disk:\n",
        "1. Defines the file path for the DeepLabV3+ model\n",
        "2. Saves the complete checkpoint with all necessary information\n",
        "3. Confirms successful saving with a print statement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50020ef7",
      "metadata": {
        "id": "50020ef7"
      },
      "outputs": [],
      "source": [
        "# Save the DeeplabV3+ model after test evaluation\n",
        "deeplabv3_model_path = os.path.join(models_dir, 'deeplabv3_efficientnet_b0.pth')\n",
        "torch.save({\n",
        "    'model_state_dict': deeplabv3_model_trained.state_dict(),\n",
        "    'optimizer_state_dict': deeplabv3_optimizer.state_dict(),\n",
        "    'history': deeplabv3_history\n",
        "}, deeplabv3_model_path)\n",
        "print(f\"DeeplabV3+ model saved to {deeplabv3_model_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4509ff98",
      "metadata": {
        "id": "4509ff98"
      },
      "source": [
        "### 6.9 Loading the DeepLabV3+ Model\n",
        "\n",
        "This cell defines a function to load the saved DeepLabV3+ model:\n",
        "1. Implements the `load_deeplabv3_model` function with special handling for the two-component architecture:\n",
        "   - First initializes a fresh EfficientNet-B0 base model\n",
        "   - Then creates the DeepLabV3+ model with that base\n",
        "   - Loads the saved weights and states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bafe165f",
      "metadata": {
        "id": "bafe165f"
      },
      "outputs": [],
      "source": [
        "# Load the DeeplabV3+ model\n",
        "def load_deeplabv3_model(model_path):\n",
        "    base_model = EfficientNet.from_pretrained('efficientnet-b0')  # We need a base model for DeeplabV3+\n",
        "    model = DeepLabV3Plus(base_model).to(device)\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    history = checkpoint['history']\n",
        "    print(f\"Loaded DeeplabV3+ model.\")\n",
        "    return model, history\n",
        "\n",
        "# Example usage:\n",
        "loaded_deeplabv3_model, loaded_deeplabv3_history = load_deeplabv3_model(deeplabv3_model_path)\n",
        "# The loaded model can now be used for inference"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44028095",
      "metadata": {
        "id": "44028095"
      },
      "source": [
        "### 6.10 Saving the Combined Model\n",
        "\n",
        "This cell saves the trained combined model to disk:\n",
        "1. Defines the file path for the combined model\n",
        "2. Saves a comprehensive checkpoint including model weights, optimizer state, history\n",
        "3. Confirms successful saving with a print statement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d337737",
      "metadata": {
        "id": "3d337737"
      },
      "outputs": [],
      "source": [
        "# Save the combined model after test evaluation\n",
        "combined_model_path = os.path.join(models_dir, 'combined_model_efficientnet_b0.pth')\n",
        "torch.save({\n",
        "    'model_state_dict': combined_model_trained.state_dict(),\n",
        "    'optimizer_state_dict': combined_optimizer.state_dict(),\n",
        "    'history': combined_history,\n",
        "}, combined_model_path)\n",
        "print(f\"Combined model saved to {combined_model_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c8712a0",
      "metadata": {
        "id": "7c8712a0"
      },
      "source": [
        "### 6.11 Loading the Combined Model\n",
        "\n",
        "This cell defines a function to load the saved combined model:\n",
        "1. Implements the `load_combined_model` function that initializes the architecture with all modifications\n",
        "2. Loads the saved weights and states\n",
        "3. Provides an example of loading the model for future use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9e4d7ba",
      "metadata": {
        "id": "d9e4d7ba"
      },
      "outputs": [],
      "source": [
        "# Load the combined model\n",
        "def load_combined_model(model_path):\n",
        "    model = CombinedModel().to(device)\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    history = checkpoint['history']\n",
        "    print(f\"Loaded combined model.\")\n",
        "    return model, history, test_acc, test_loss\n",
        "\n",
        "# Example usage:\n",
        "loaded_combined_model, loaded_combined_history = load_combined_model(combined_model_path)\n",
        "# The loaded model can now be used for inference"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}