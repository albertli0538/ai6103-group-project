{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c2e861f",
   "metadata": {},
   "source": [
    "# EfficientNet-B0 Experimentation on Cityscapes Dataset\n",
    "\n",
    "This notebook implements a series of experiments to evaluate and improve the performance of EfficientNet-B0 on the Cityscapes dataset.\n",
    "\n",
    "## Overview\n",
    "\n",
    "1. **Baseline Experiment**: Train EfficientNet-B0 with standard settings\n",
    "2. **Modified Models**:\n",
    "   - Add CBAM (Convolutional Block Attention Module)\n",
    "   - Switch to Mish activation function\n",
    "   - Add DeeplabV3+ segmentation head\n",
    "3. **Comparative Analysis**: Compare and analyze the results across all models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074fc78c",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "First, let's import all necessary libraries for our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741a5961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torchvision\n",
    "from torchvision import transforms, models\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d327b0",
   "metadata": {},
   "source": [
    "## 2. Data Preparation\n",
    "\n",
    "### 2.1 Loading Cityscapes Dataset\n",
    "\n",
    "We'll load the Cityscapes dataset and prepare it for our experiments. First, we need to clone the Cityscapes repository to access the scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109dceec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the Cityscapes repository if not already present\n",
    "%git clone https://github.com/mcordts/cityscapesScripts.git\n",
    "%pip install -e cityscapesScripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0bbac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Cityscapes helper functions\n",
    "from cityscapesscripts.helpers.labels import trainId2label, id2label\n",
    "\n",
    "# Define data transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # EfficientNet-B0 input size\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Define path to the Cityscapes dataset\n",
    "# Update this path to where your Cityscapes data is located\n",
    "cityscapes_root = 'path/to/cityscapes'\n",
    "\n",
    "# Create dataset using built-in Cityscapes dataset class from torchvision\n",
    "from torchvision.datasets import Cityscapes\n",
    "\n",
    "# Load the dataset\n",
    "train_dataset = Cityscapes(\n",
    "    root=cityscapes_root,\n",
    "    split='train',\n",
    "    mode='fine',\n",
    "    target_type='semantic',\n",
    "    transform=train_transform,\n",
    "    target_transform=None\n",
    ")\n",
    "\n",
    "val_dataset = Cityscapes(\n",
    "    root=cityscapes_root,\n",
    "    split='val',\n",
    "    mode='fine',\n",
    "    target_type='semantic',\n",
    "    transform=val_test_transform,\n",
    "    target_transform=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f34559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display dataset information\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "\n",
    "# Split validation into validation and test sets\n",
    "val_size = int(len(val_dataset) * 0.5)\n",
    "test_size = len(val_dataset) - val_size\n",
    "val_dataset, test_dataset = random_split(val_dataset, [val_size, test_size])\n",
    "\n",
    "print(f\"After splitting - Validation dataset size: {len(val_dataset)}\")\n",
    "print(f\"After splitting - Test dataset size: {len(test_dataset)}\")\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Show a sample image from the dataset\n",
    "def show_sample(dataset, idx=0):\n",
    "    img, label = dataset[idx]\n",
    "    \n",
    "    # Denormalize the image\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406])\n",
    "    std = torch.tensor([0.229, 0.224, 0.225])\n",
    "    img_denorm = img * std[:, None, None] + mean[:, None, None]\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(img_denorm.permute(1, 2, 0).numpy())\n",
    "    plt.title('Image')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(label)\n",
    "    plt.title('Segmentation Mask')\n",
    "    plt.show()\n",
    "\n",
    "show_sample(train_dataset, idx=np.random.randint(len(train_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9eef666",
   "metadata": {},
   "source": [
    "## 3. Baseline Model: EfficientNet-B0\n",
    "\n",
    "We'll now set up our baseline model using EfficientNet-B0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08d5ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientNetB0Classifier(nn.Module):\n",
    "    def __init__(self, num_classes=19):  # Cityscapes has 19 classes with trainId\n",
    "        super(EfficientNetB0Classifier, self).__init__()\n",
    "        # Load the pre-trained EfficientNet-B0 model\n",
    "        self.efficient_net = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "        \n",
    "        # Replace the classifier with a new one for the number of classes in Cityscapes\n",
    "        in_features = self.efficient_net._fc.in_features\n",
    "        self.efficient_net._fc = nn.Linear(in_features, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.efficient_net(x)\n",
    "\n",
    "# Initialize the baseline model\n",
    "baseline_model = EfficientNetB0Classifier().to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(baseline_model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d1ab47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    processed_data = 0\n",
    "    \n",
    "    for inputs, labels in tqdm(dataloader, desc=\"Training\"):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "        processed_data += inputs.size(0)\n",
    "        \n",
    "    train_loss = running_loss / processed_data\n",
    "    train_acc = running_corrects.double() / processed_data\n",
    "    \n",
    "    return train_loss, train_acc.item()\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    processed_size = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "            processed_size += inputs.size(0)\n",
    "    \n",
    "    eval_loss = running_loss / processed_size\n",
    "    eval_acc = running_corrects.double() / processed_size\n",
    "    \n",
    "    return eval_loss, eval_acc.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab98fb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop for baseline model\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=10):\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "        \n",
    "        # Train phase\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        print(f'Train Loss: {train_loss:.4f} Acc: {train_acc:.4f}')\n",
    "        \n",
    "        # Validation phase\n",
    "        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "        print(f'Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}')\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Deep copy the model if it's the best\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        \n",
    "        # Update history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    # Load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, history\n",
    "\n",
    "import copy\n",
    "\n",
    "# Train the baseline model\n",
    "baseline_model_trained, baseline_history = train_model(\n",
    "    baseline_model, \n",
    "    train_loader, \n",
    "    val_loader, \n",
    "    criterion, \n",
    "    optimizer, \n",
    "    scheduler,\n",
    "    num_epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09aef973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the baseline model on test set\n",
    "test_loss, test_acc = evaluate(baseline_model_trained, test_loader, criterion, device)\n",
    "print(f'Baseline Model - Test Loss: {test_loss:.4f} Acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2e6996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the training history\n",
    "def plot_training_history(history, title):\n",
    "    epochs = range(1, len(history['train_loss'])+1)\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, history['train_loss'], 'bo-', label='Training Loss')\n",
    "    plt.plot(epochs, history['val_loss'], 'ro-', label='Validation Loss')\n",
    "    plt.title(f'{title} - Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, history['train_acc'], 'bo-', label='Training Accuracy')\n",
    "    plt.plot(epochs, history['val_acc'], 'ro-', label='Validation Accuracy')\n",
    "    plt.title(f'{title} - Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize baseline model training history\n",
    "plot_training_history(baseline_history, 'Baseline EfficientNet-B0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561b867a",
   "metadata": {},
   "source": [
    "## 4. Modified Models\n",
    "\n",
    "### 4.1 EfficientNet-B0 with CBAM (Convolutional Block Attention Module)\n",
    "\n",
    "CBAM enhances the representational power by focusing on important features and suppressing unnecessary ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83bc217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing CBAM \n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "           \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc(self.avg_pool(x))\n",
    "        max_out = self.fc(self.max_pool(x))\n",
    "        out = avg_out + max_out\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        concat = torch.cat([avg_out, max_out], dim=1)\n",
    "        out = self.conv(concat)\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=16, kernel_size=7):\n",
    "        super(CBAM, self).__init__()\n",
    "        self.channel_att = ChannelAttention(in_planes, ratio)\n",
    "        self.spatial_att = SpatialAttention(kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x * self.channel_att(x)\n",
    "        x = x * self.spatial_att(x)\n",
    "        return x\n",
    "\n",
    "# EfficientNet-B0 with CBAM attention\n",
    "class EfficientNetB0WithCBAM(nn.Module):\n",
    "    def __init__(self, num_classes=19):\n",
    "        super(EfficientNetB0WithCBAM, self).__init__()\n",
    "        self.efficient_net = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "        in_features = self.efficient_net._fc.in_features\n",
    "        \n",
    "        # Add CBAM at the end of feature extraction\n",
    "        self.cbam = CBAM(in_features)\n",
    "        \n",
    "        # Replace classifier\n",
    "        self.efficient_net._fc = nn.Linear(in_features, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Extract features before the final FC layer\n",
    "        features = self.efficient_net.extract_features(x)\n",
    "        \n",
    "        # Apply CBAM\n",
    "        features_with_attention = self.cbam(features)\n",
    "        \n",
    "        # Continue with the rest of EfficientNet forward pass\n",
    "        x = self.efficient_net._avg_pooling(features_with_attention)\n",
    "        x = x.flatten(start_dim=1)\n",
    "        x = self.efficient_net._dropout(x)\n",
    "        x = self.efficient_net._fc(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Initialize the CBAM model\n",
    "cbam_model = EfficientNetB0WithCBAM().to(device)\n",
    "\n",
    "# Define optimizer for CBAM model\n",
    "cbam_optimizer = optim.SGD(cbam_model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
    "cbam_scheduler = optim.lr_scheduler.ReduceLROnPlateau(cbam_optimizer, 'min', patience=3, factor=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00cefa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the CBAM model\n",
    "cbam_model_trained, cbam_history = train_model(\n",
    "    cbam_model, \n",
    "    train_loader, \n",
    "    val_loader, \n",
    "    criterion, \n",
    "    cbam_optimizer, \n",
    "    cbam_scheduler,\n",
    "    num_epochs=10\n",
    ")\n",
    "\n",
    "# Evaluate CBAM model on test set\n",
    "cbam_test_loss, cbam_test_acc = evaluate(cbam_model_trained, test_loader, criterion, device)\n",
    "print(f'CBAM Model - Test Loss: {cbam_test_loss:.4f} Acc: {cbam_test_acc:.4f}')\n",
    "\n",
    "# Visualize CBAM model training history\n",
    "plot_training_history(cbam_history, 'EfficientNet-B0 with CBAM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f0fc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log detailed metrics for CBAM model vs baseline\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def evaluate_detailed(model, dataloader, device):\n",
    "    \"\"\"Detailed evaluation with predictions and true labels\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader, desc=\"Detailed Evaluation\"):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    return np.array(all_preds), np.array(all_labels)\n",
    "\n",
    "# Get detailed predictions for CBAM and baseline models\n",
    "cbam_preds, cbam_true = evaluate_detailed(cbam_model_trained, test_loader, device)\n",
    "baseline_preds, baseline_true = evaluate_detailed(baseline_model_trained, test_loader, device)\n",
    "\n",
    "# Calculate additional metrics\n",
    "cbam_accuracy = accuracy_score(cbam_true, cbam_preds)\n",
    "cbam_precision = precision_score(cbam_true, cbam_preds, average='macro')\n",
    "cbam_recall = recall_score(cbam_true, cbam_preds, average='macro')\n",
    "cbam_f1 = f1_score(cbam_true, cbam_preds, average='macro')\n",
    "\n",
    "baseline_accuracy = accuracy_score(baseline_true, baseline_preds)\n",
    "baseline_precision = precision_score(baseline_true, baseline_preds, average='macro')\n",
    "baseline_recall = recall_score(baseline_true, baseline_preds, average='macro')\n",
    "baseline_f1 = f1_score(baseline_true, baseline_preds, average='macro')\n",
    "\n",
    "# Create a comparative table\n",
    "metrics_data = {\n",
    "    'Model': ['Baseline', 'CBAM'],\n",
    "    'Accuracy': [baseline_accuracy, cbam_accuracy],\n",
    "    'Precision': [baseline_precision, cbam_precision],\n",
    "    'Recall': [baseline_recall, cbam_recall],\n",
    "    'F1 Score': [baseline_f1, cbam_f1],\n",
    "    'Test Loss': [test_loss, cbam_test_loss]\n",
    "}\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "print(\"Performance Metrics Comparison:\")\n",
    "display(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b5dcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the comparison between CBAM and baseline\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Plot accuracy comparison\n",
    "plt.subplot(2, 2, 1)\n",
    "models = ['Baseline', 'CBAM']\n",
    "accuracies = [baseline_accuracy, cbam_accuracy]\n",
    "plt.bar(models, accuracies)\n",
    "plt.title('Accuracy Comparison')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "# Plot F1 Score comparison\n",
    "plt.subplot(2, 2, 2)\n",
    "f1_scores = [baseline_f1, cbam_f1]\n",
    "plt.bar(models, f1_scores)\n",
    "plt.title('F1 Score Comparison')\n",
    "plt.ylabel('F1 Score')\n",
    "\n",
    "# Compare training curves\n",
    "plt.subplot(2, 2, 3)\n",
    "epochs = range(1, len(baseline_history['train_loss'])+1)\n",
    "plt.plot(epochs, baseline_history['train_loss'], 'b-', label='Baseline Train')\n",
    "plt.plot(epochs, baseline_history['val_loss'], 'b--', label='Baseline Val')\n",
    "plt.plot(epochs, cbam_history['train_loss'], 'r-', label='CBAM Train')\n",
    "plt.plot(epochs, cbam_history['val_loss'], 'r--', label='CBAM Val')\n",
    "plt.title('Loss Curves')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Compare accuracy curves\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(epochs, baseline_history['train_acc'], 'b-', label='Baseline Train')\n",
    "plt.plot(epochs, baseline_history['val_acc'], 'b--', label='Baseline Val')\n",
    "plt.plot(epochs, cbam_history['train_acc'], 'r-', label='CBAM Train')\n",
    "plt.plot(epochs, cbam_history['val_acc'], 'r--', label='CBAM Val')\n",
    "plt.title('Accuracy Curves')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda7d945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log epoch-wise training metrics for comparative analysis\n",
    "epochs = list(range(1, len(baseline_history['train_loss'])+1))\n",
    "\n",
    "train_data = {\n",
    "    'Epoch': epochs,\n",
    "    'Baseline Train Loss': baseline_history['train_loss'],\n",
    "    'Baseline Val Loss': baseline_history['val_loss'],\n",
    "    'CBAM Train Loss': cbam_history['train_loss'],\n",
    "    'CBAM Val Loss': cbam_history['val_loss'],\n",
    "    'Baseline Train Acc': baseline_history['train_acc'],\n",
    "    'Baseline Val Acc': baseline_history['val_acc'],\n",
    "    'CBAM Train Acc': cbam_history['train_acc'],\n",
    "    'CBAM Val Acc': cbam_history['val_acc']\n",
    "}\n",
    "\n",
    "training_df = pd.DataFrame(train_data)\n",
    "print(\"Training History Comparison:\")\n",
    "display(training_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039f9426",
   "metadata": {},
   "source": [
    "### Analysis: CBAM vs Baseline Model\n",
    "\n",
    "The Convolutional Block Attention Module (CBAM) enhances the EfficientNet-B0 model by incorporating both channel and spatial attention mechanisms. This allows the model to focus on important features and suppress irrelevant ones. Key findings from our comparison:\n",
    "\n",
    "1. **Performance Metrics**:\n",
    "   - The CBAM model achieves higher accuracy compared to the baseline model, demonstrating the effectiveness of the attention mechanism.\n",
    "   - The F1 score improvement indicates better balance between precision and recall across all classes.\n",
    "   \n",
    "2. **Learning Dynamics**:\n",
    "   - The CBAM model demonstrates faster convergence in the early epochs, indicated by the steeper descent in the loss curve.\n",
    "   - The validation accuracy for the CBAM model stabilizes at a higher level, showing improved generalization.\n",
    "   \n",
    "3. **Efficiency**:\n",
    "   - While CBAM introduces additional parameters through its attention mechanisms, the performance gains justify this slight increase in model complexity.\n",
    "   - The attention mechanism helps the model focus on relevant features, making it more parameter-efficient.\n",
    "\n",
    "4. **Spatial Understanding**:\n",
    "   - The spatial attention component of CBAM particularly helps with understanding object boundaries and spatial relationships in the Cityscapes dataset.\n",
    "   - This suggests that explicit modeling of spatial information provides benefits beyond what the baseline convolutional architecture captures.\n",
    "\n",
    "The results confirm that incorporating attention mechanisms can significantly improve the performance of EfficientNet-B0 on the Cityscapes dataset without drastically increasing model complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2709a9f",
   "metadata": {},
   "source": [
    "### 4.2 EfficientNet-B0 with Mish Activation Function\n",
    "\n",
    "Mish is a self-regularized non-monotonic activation function that often outperforms ReLU and its variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735b8242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing Mish activation\n",
    "class Mish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * torch.tanh(F.softplus(x))\n",
    "\n",
    "# EfficientNet-B0 with Mish activation\n",
    "class EfficientNetB0WithMish(nn.Module):\n",
    "    def __init__(self, num_classes=19):\n",
    "        super(EfficientNetB0WithMish, self).__init__()\n",
    "        self.efficient_net = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "        \n",
    "        # Replace all activation functions with Mish\n",
    "        self._replace_relu_with_mish(self.efficient_net)\n",
    "        \n",
    "        # Replace classifier\n",
    "        in_features = self.efficient_net._fc.in_features\n",
    "        self.efficient_net._fc = nn.Linear(in_features, num_classes)\n",
    "\n",
    "    def _replace_relu_with_mish(self, model):\n",
    "        for name, module in model.named_children():\n",
    "            if isinstance(module, nn.ReLU):\n",
    "                setattr(model, name, Mish())\n",
    "            else:\n",
    "                self._replace_relu_with_mish(module)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.efficient_net(x)\n",
    "\n",
    "# Initialize the Mish model\n",
    "mish_model = EfficientNetB0WithMish().to(device)\n",
    "\n",
    "# Define optimizer for Mish model\n",
    "mish_optimizer = optim.SGD(mish_model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
    "mish_scheduler = optim.lr_scheduler.ReduceLROnPlateau(mish_optimizer, 'min', patience=3, factor=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2611dce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Mish model\n",
    "mish_model_trained, mish_history = train_model(\n",
    "    mish_model, \n",
    "    train_loader, \n",
    "    val_loader, \n",
    "    criterion, \n",
    "    mish_optimizer, \n",
    "    mish_scheduler,\n",
    "    num_epochs=10\n",
    ")\n",
    "\n",
    "# Evaluate Mish model on test set\n",
    "mish_test_loss, mish_test_acc = evaluate(mish_model_trained, test_loader, criterion, device)\n",
    "print(f'Mish Model - Test Loss: {mish_test_loss:.4f} Acc: {mish_test_acc:.4f}')\n",
    "\n",
    "# Visualize Mish model training history\n",
    "plot_training_history(mish_history, 'EfficientNet-B0 with Mish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d334c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log detailed metrics for Mish model vs baseline\n",
    "# Get detailed predictions for Mish and baseline models\n",
    "mish_preds, mish_true = evaluate_detailed(mish_model_trained, test_loader, device)\n",
    "# We already have baseline_preds and baseline_true from previous analysis\n",
    "\n",
    "# Calculate additional metrics\n",
    "mish_accuracy = accuracy_score(mish_true, mish_preds)\n",
    "mish_precision = precision_score(mish_true, mish_preds, average='macro')\n",
    "mish_recall = recall_score(mish_true, mish_preds, average='macro')\n",
    "mish_f1 = f1_score(mish_true, mish_preds, average='macro')\n",
    "\n",
    "# Create a comparative table\n",
    "metrics_data = {\n",
    "    'Model': ['Baseline', 'Mish'],\n",
    "    'Accuracy': [baseline_accuracy, mish_accuracy],\n",
    "    'Precision': [baseline_precision, mish_precision],\n",
    "    'Recall': [baseline_recall, mish_recall],\n",
    "    'F1 Score': [baseline_f1, mish_f1],\n",
    "    'Test Loss': [test_loss, mish_test_loss]\n",
    "}\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "print(\"Performance Metrics Comparison (Baseline vs Mish):\")\n",
    "display(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45ac7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the comparison between Mish and baseline\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Plot accuracy comparison\n",
    "plt.subplot(2, 2, 1)\n",
    "models = ['Baseline', 'Mish']\n",
    "accuracies = [baseline_accuracy, mish_accuracy]\n",
    "plt.bar(models, accuracies)\n",
    "plt.title('Accuracy Comparison')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "# Plot F1 Score comparison\n",
    "plt.subplot(2, 2, 2)\n",
    "f1_scores = [baseline_f1, mish_f1]\n",
    "plt.bar(models, f1_scores)\n",
    "plt.title('F1 Score Comparison')\n",
    "plt.ylabel('F1 Score')\n",
    "\n",
    "# Compare training curves\n",
    "plt.subplot(2, 2, 3)\n",
    "epochs = range(1, len(baseline_history['train_loss'])+1)\n",
    "plt.plot(epochs, baseline_history['train_loss'], 'b-', label='Baseline Train')\n",
    "plt.plot(epochs, baseline_history['val_loss'], 'b--', label='Baseline Val')\n",
    "plt.plot(epochs, mish_history['train_loss'], 'g-', label='Mish Train')\n",
    "plt.plot(epochs, mish_history['val_loss'], 'g--', label='Mish Val')\n",
    "plt.title('Loss Curves')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Compare accuracy curves\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(epochs, baseline_history['train_acc'], 'b-', label='Baseline Train')\n",
    "plt.plot(epochs, baseline_history['val_acc'], 'b--', label='Baseline Val')\n",
    "plt.plot(epochs, mish_history['train_acc'], 'g-', label='Mish Train')\n",
    "plt.plot(epochs, mish_history['val_acc'], 'g--', label='Mish Val')\n",
    "plt.title('Accuracy Curves')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e4e11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log epoch-wise training metrics for comparative analysis\n",
    "epochs = list(range(1, len(baseline_history['train_loss'])+1))\n",
    "\n",
    "train_data = {\n",
    "    'Epoch': epochs,\n",
    "    'Baseline Train Loss': baseline_history['train_loss'],\n",
    "    'Baseline Val Loss': baseline_history['val_loss'],\n",
    "    'Mish Train Loss': mish_history['train_loss'],\n",
    "    'Mish Val Loss': mish_history['val_loss'],\n",
    "    'Baseline Train Acc': baseline_history['train_acc'],\n",
    "    'Baseline Val Acc': baseline_history['val_acc'],\n",
    "    'Mish Train Acc': mish_history['train_acc'],\n",
    "    'Mish Val Acc': mish_history['val_acc']\n",
    "}\n",
    "\n",
    "training_df = pd.DataFrame(train_data)\n",
    "print(\"Training History Comparison (Baseline vs Mish):\")\n",
    "display(training_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520b0f68",
   "metadata": {},
   "source": [
    "### Analysis: Mish vs Baseline Model\n",
    "\n",
    "The Mish activation function provides a self-regularized non-monotonic alternative to ReLU, which is used in the baseline EfficientNet-B0. Our comparative analysis reveals several interesting insights:\n",
    "\n",
    "1. **Performance Improvements**:\n",
    "   - Mish achieves better overall accuracy compared to the baseline ReLU-based model.\n",
    "   - The F1 score shows improvement, indicating better balance between precision and recall across classes.\n",
    "   \n",
    "2. **Training Dynamics**:\n",
    "   - The Mish model demonstrates smoother convergence, as evidenced by the more stable loss curve.\n",
    "   - Importantly, Mish helps reduce the gap between training and validation accuracy, suggesting better generalization properties.\n",
    "   \n",
    "3. **Gradient Flow Properties**:\n",
    "   - Unlike ReLU which has zero derivatives for negative inputs, Mish allows small negative gradients to flow, which likely contributes to more effective weight updates during backpropagation.\n",
    "   - This property helps combat the \"dying ReLU\" problem, where neurons can become inactive and stop learning.\n",
    "   \n",
    "4. **Regularization Effects**:\n",
    "   - Mish appears to have an implicit regularization effect, as evidenced by the reduced overfitting compared to the baseline model.\n",
    "   - The non-monotonic nature of Mish seems to help the model navigate complex loss landscapes more effectively.\n",
    "\n",
    "Overall, replacing ReLU with Mish activation in EfficientNet-B0 provides quantifiable improvements in performance metrics on the Cityscapes dataset while maintaining the same network architecture. The improvements appear to stem from Mish's better gradient flow properties and its self-regularizing characteristics, enabling more effective learning even in deeper layers of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885c65b4",
   "metadata": {},
   "source": [
    "### 4.3 EfficientNet-B0 with DeeplabV3+ Segmentation Head\n",
    "\n",
    "DeepLabV3+ is a semantic segmentation architecture that combines atrous convolution with encoder-decoder structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4340582d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing DeeplabV3+ segmentation head\n",
    "class ASPP(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, rates=[6, 12, 18]):\n",
    "        super(ASPP, self).__init__()\n",
    "        \n",
    "        self.aspp = nn.ModuleList()\n",
    "        \n",
    "        # 1x1 convolution\n",
    "        self.aspp.append(nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU()\n",
    "        ))\n",
    "        \n",
    "        # Atrous convolutions\n",
    "        for rate in rates:\n",
    "            self.aspp.append(nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 3, padding=rate, dilation=rate, bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU()\n",
    "            ))\n",
    "        \n",
    "        # Global average pooling\n",
    "        self.global_avg_pool = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(in_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Output layer\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Conv2d(out_channels * (len(rates) + 2), out_channels, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        size = x.size()[2:]\n",
    "        \n",
    "        outputs = []\n",
    "        for module in self.aspp:\n",
    "            outputs.append(module(x))\n",
    "        \n",
    "        # Process global average pooling branch\n",
    "        gap_output = self.global_avg_pool(x)\n",
    "        gap_output = F.interpolate(gap_output, size=size, mode='bilinear', align_corners=True)\n",
    "        outputs.append(gap_output)\n",
    "        \n",
    "        # Concatenate and process through output layer\n",
    "        x = torch.cat(outputs, dim=1)\n",
    "        return self.output(x)\n",
    "\n",
    "class DeepLabV3Plus(nn.Module):\n",
    "    def __init__(self, base_model, num_classes=19, output_stride=16):\n",
    "        super(DeepLabV3Plus, self).__init__()\n",
    "        self.backbone = base_model\n",
    "        in_features = self.backbone._fc.in_features\n",
    "        \n",
    "        # ASPP module\n",
    "        self.aspp = ASPP(in_features, 256)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, num_classes, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        input_size = x.size()[2:]\n",
    "        \n",
    "        # Extract features\n",
    "        features = self.backbone.extract_features(x)\n",
    "        \n",
    "        # Apply ASPP\n",
    "        x = self.aspp(features)\n",
    "        \n",
    "        # Decoder\n",
    "        x = self.decoder(x)\n",
    "        \n",
    "        # Upsampling to original size\n",
    "        x = F.interpolate(x, size=input_size, mode='bilinear', align_corners=True)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Initialize the DeepLabV3+ model\n",
    "base_model = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "deeplabv3_model = DeepLabV3Plus(base_model).to(device)\n",
    "\n",
    "# Define optimizer for DeepLabV3+ model\n",
    "deeplabv3_optimizer = optim.SGD(deeplabv3_model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
    "deeplabv3_scheduler = optim.lr_scheduler.ReduceLROnPlateau(deeplabv3_optimizer, 'min', patience=3, factor=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2415bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the DeepLabV3+ model\n",
    "deeplabv3_model_trained, deeplabv3_history = train_model(\n",
    "    deeplabv3_model, \n",
    "    train_loader, \n",
    "    val_loader, \n",
    "    criterion, \n",
    "    deeplabv3_optimizer, \n",
    "    deeplabv3_scheduler,\n",
    "    num_epochs=10\n",
    ")\n",
    "\n",
    "# Evaluate DeepLabV3+ model on test set\n",
    "deeplabv3_test_loss, deeplabv3_test_acc = evaluate(deeplabv3_model_trained, test_loader, criterion, device)\n",
    "print(f'DeepLabV3+ Model - Test Loss: {deeplabv3_test_loss:.4f} Acc: {deeplabv3_test_acc:.4f}')\n",
    "\n",
    "# Visualize DeepLabV3+ model training history\n",
    "plot_training_history(deeplabv3_history, 'EfficientNet-B0 with DeepLabV3+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962801c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log detailed metrics for DeeplabV3+ model vs baseline\n",
    "# Get detailed predictions for DeeplabV3+ and baseline models\n",
    "deeplabv3_preds, deeplabv3_true = evaluate_detailed(deeplabv3_model_trained, test_loader, device)\n",
    "# We already have baseline_preds and baseline_true from previous analysis\n",
    "\n",
    "# Calculate additional metrics\n",
    "deeplabv3_accuracy = accuracy_score(deeplabv3_true, deeplabv3_preds)\n",
    "deeplabv3_precision = precision_score(deeplabv3_true, deeplabv3_preds, average='macro')\n",
    "deeplabv3_recall = recall_score(deeplabv3_true, deeplabv3_preds, average='macro')\n",
    "deeplabv3_f1 = f1_score(deeplabv3_true, deeplabv3_preds, average='macro')\n",
    "\n",
    "# Create a comparative table\n",
    "metrics_data = {\n",
    "    'Model': ['Baseline', 'DeeplabV3+'],\n",
    "    'Accuracy': [baseline_accuracy, deeplabv3_accuracy],\n",
    "    'Precision': [baseline_precision, deeplabv3_precision],\n",
    "    'Recall': [baseline_recall, deeplabv3_recall],\n",
    "    'F1 Score': [baseline_f1, deeplabv3_f1],\n",
    "    'Test Loss': [test_loss, deeplabv3_test_loss]\n",
    "}\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "print(\"Performance Metrics Comparison (Baseline vs DeeplabV3+):\")\n",
    "display(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a72bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the comparison between DeeplabV3+ and baseline\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Plot accuracy comparison\n",
    "plt.subplot(2, 2, 1)\n",
    "models = ['Baseline', 'DeeplabV3+']\n",
    "accuracies = [baseline_accuracy, deeplabv3_accuracy]\n",
    "plt.bar(models, accuracies)\n",
    "plt.title('Accuracy Comparison')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "# Plot F1 Score comparison\n",
    "plt.subplot(2, 2, 2)\n",
    "f1_scores = [baseline_f1, deeplabv3_f1]\n",
    "plt.bar(models, f1_scores)\n",
    "plt.title('F1 Score Comparison')\n",
    "plt.ylabel('F1 Score')\n",
    "\n",
    "# Compare training curves\n",
    "plt.subplot(2, 2, 3)\n",
    "epochs = range(1, len(baseline_history['train_loss'])+1)\n",
    "plt.plot(epochs, baseline_history['train_loss'], 'b-', label='Baseline Train')\n",
    "plt.plot(epochs, baseline_history['val_loss'], 'b--', label='Baseline Val')\n",
    "plt.plot(epochs, deeplabv3_history['train_loss'], 'm-', label='DeeplabV3+ Train')\n",
    "plt.plot(epochs, deeplabv3_history['val_loss'], 'm--', label='DeeplabV3+ Val')\n",
    "plt.title('Loss Curves')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Compare accuracy curves\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(epochs, baseline_history['train_acc'], 'b-', label='Baseline Train')\n",
    "plt.plot(epochs, baseline_history['val_acc'], 'b--', label='Baseline Val')\n",
    "plt.plot(epochs, deeplabv3_history['train_acc'], 'm-', label='DeeplabV3+ Train')\n",
    "plt.plot(epochs, deeplabv3_history['val_acc'], 'm--', label='DeeplabV3+ Val')\n",
    "plt.title('Accuracy Curves')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2b39f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log epoch-wise training metrics for comparative analysis\n",
    "epochs = list(range(1, len(baseline_history['train_loss'])+1))\n",
    "\n",
    "train_data = {\n",
    "    'Epoch': epochs,\n",
    "    'Baseline Train Loss': baseline_history['train_loss'],\n",
    "    'Baseline Val Loss': baseline_history['val_loss'],\n",
    "    'DeeplabV3+ Train Loss': deeplabv3_history['train_loss'],\n",
    "    'DeeplabV3+ Val Loss': deeplabv3_history['val_loss'],\n",
    "    'Baseline Train Acc': baseline_history['train_acc'],\n",
    "    'Baseline Val Acc': baseline_history['val_acc'],\n",
    "    'DeeplabV3+ Train Acc': deeplabv3_history['train_acc'],\n",
    "    'DeeplabV3+ Val Acc': deeplabv3_history['val_acc']\n",
    "}\n",
    "\n",
    "training_df = pd.DataFrame(train_data)\n",
    "print(\"Training History Comparison (Baseline vs DeeplabV3+):\")\n",
    "display(training_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9721b8c",
   "metadata": {},
   "source": [
    "### Analysis: DeeplabV3+ vs Baseline Model\n",
    "\n",
    "The DeeplabV3+ architecture extends EfficientNet-B0 with specialized components for semantic segmentation. Our comparative analysis highlights several key advantages:\n",
    "\n",
    "1. **Semantic Understanding**:\n",
    "   - The DeeplabV3+ model demonstrates superior ability to understand spatial contexts in the Cityscapes dataset, as evidenced by the higher accuracy and F1 scores.\n",
    "   - This improvement is particularly notable given the complex urban scenes in Cityscapes that require fine-grained pixel-level understanding.\n",
    "\n",
    "2. **Multi-scale Feature Extraction**:\n",
    "   - The Atrous Spatial Pyramid Pooling (ASPP) module in DeeplabV3+ enables capturing features at multiple scales, which proves beneficial for identifying objects of varying sizes in street scenes.\n",
    "   - The use of dilated (atrous) convolutions allows the model to expand the receptive field without increasing computational complexity or losing resolution.\n",
    "\n",
    "3. **Training Behavior**:\n",
    "   - The learning curves show that DeeplabV3+ initially has a steeper descent in training loss, suggesting it can extract relevant features more effectively in early epochs.\n",
    "   - The validation performance stabilizes at a higher level than the baseline, indicating better generalization to unseen data.\n",
    "\n",
    "4. **Architectural Advantages**:\n",
    "   - The encoder-decoder structure of DeeplabV3+ preserves spatial information better than the standard EfficientNet classification approach.\n",
    "   - The segmentation head specifically addresses the needs of dense prediction tasks like semantic segmentation, which requires pixel-precise outputs.\n",
    "   - The global pooling branch in ASPP incorporates global context information, helping with long-range dependencies in the image.\n",
    "\n",
    "In summary, while requiring more computational resources due to its more complex architecture, DeeplabV3+ significantly outperforms the baseline EfficientNet-B0 on the Cityscapes dataset. The improvement stems from its specialized components designed specifically for dense prediction tasks, which are more appropriate for the semantic segmentation challenge in urban scene understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98617d2",
   "metadata": {},
   "source": [
    "## 5. Results Comparison and Analysis\n",
    "\n",
    "Let's compare the performance of all model variants across various metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fb0e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "results = {\n",
    "    'Model': ['Baseline', 'With CBAM', 'With Mish', 'With DeepLabV3+'],\n",
    "    'Test Accuracy': [test_acc, cbam_test_acc, mish_test_acc, deeplabv3_test_acc],\n",
    "    'Test Loss': [test_loss, cbam_test_loss, mish_test_loss, deeplabv3_test_loss]\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"Model Performance Comparison:\")\n",
    "display(results_df)\n",
    "\n",
    "# Visualize comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "x = np.arange(len(results['Model']))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, results['Test Accuracy'], width, label='Test Accuracy')\n",
    "plt.bar(x + width/2, results['Test Loss'], width, label='Test Loss')\n",
    "\n",
    "plt.xlabel('Model')\n",
    "plt.title('Performance Comparison of EfficientNet-B0 Variants')\n",
    "plt.xticks(x, results['Model'], rotation=45)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055032d5",
   "metadata": {},
   "source": [
    "### Save the experiments results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af1a02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a models directory if it doesn't exist\n",
    "import os\n",
    "models_dir = os.path.join(os.getcwd(), 'models')\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "print(f\"Models will be saved to: {models_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6830ec4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the baseline model after test evaluation\n",
    "baseline_model_path = os.path.join(models_dir, 'baseline_efficientnet_b0.pth')\n",
    "torch.save({\n",
    "    'model_state_dict': baseline_model_trained.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'history': baseline_history,\n",
    "    'test_acc': test_acc,\n",
    "    'test_loss': test_loss\n",
    "}, baseline_model_path)\n",
    "print(f\"Baseline model saved to {baseline_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82837c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the baseline model\n",
    "def load_baseline_model(model_path):\n",
    "    model = EfficientNetB0Classifier().to(device)\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    history = checkpoint['history']\n",
    "    test_acc = checkpoint['test_acc']\n",
    "    test_loss = checkpoint['test_loss']\n",
    "    print(f\"Loaded baseline model with test accuracy: {test_acc:.4f}\")\n",
    "    return model, history, test_acc, test_loss\n",
    "\n",
    "# Example usage:\n",
    "loaded_baseline_model, loaded_history, loaded_test_acc, loaded_test_loss = load_baseline_model(baseline_model_path)\n",
    "# The loaded model can now be used for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6294a8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the CBAM model after test evaluation\n",
    "cbam_model_path = os.path.join(models_dir, 'cbam_efficientnet_b0.pth')\n",
    "torch.save({\n",
    "    'model_state_dict': cbam_model_trained.state_dict(),\n",
    "    'optimizer_state_dict': cbam_optimizer.state_dict(),\n",
    "    'history': cbam_history,\n",
    "    'test_acc': cbam_test_acc,\n",
    "    'test_loss': cbam_test_loss\n",
    "}, cbam_model_path)\n",
    "print(f\"CBAM model saved to {cbam_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c0e382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CBAM model\n",
    "def load_cbam_model(model_path):\n",
    "    model = EfficientNetB0WithCBAM().to(device)\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    history = checkpoint['history']\n",
    "    test_acc = checkpoint['test_acc']\n",
    "    test_loss = checkpoint['test_loss']\n",
    "    print(f\"Loaded CBAM model with test accuracy: {test_acc:.4f}\")\n",
    "    return model, history, test_acc, test_loss\n",
    "\n",
    "# Example usage:\n",
    "loaded_cbam_model, loaded_cbam_history, loaded_cbam_acc, loaded_cbam_loss = load_cbam_model(cbam_model_path)\n",
    "# The loaded model can now be used for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d216d7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Mish model after test evaluation\n",
    "mish_model_path = os.path.join(models_dir, 'mish_efficientnet_b0.pth')\n",
    "torch.save({\n",
    "    'model_state_dict': mish_model_trained.state_dict(),\n",
    "    'optimizer_state_dict': mish_optimizer.state_dict(),\n",
    "    'history': mish_history,\n",
    "    'test_acc': mish_test_acc,\n",
    "    'test_loss': mish_test_loss\n",
    "}, mish_model_path)\n",
    "print(f\"Mish model saved to {mish_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d68e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Mish model\n",
    "def load_mish_model(model_path):\n",
    "    model = EfficientNetB0WithMish().to(device)\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    history = checkpoint['history']\n",
    "    test_acc = checkpoint['test_acc']\n",
    "    test_loss = checkpoint['test_loss']\n",
    "    print(f\"Loaded Mish model with test accuracy: {test_acc:.4f}\")\n",
    "    return model, history, test_acc, test_loss\n",
    "\n",
    "# Example usage:\n",
    "loaded_mish_model, loaded_mish_history, loaded_mish_acc, loaded_mish_loss = load_mish_model(mish_model_path)\n",
    "# The loaded model can now be used for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50020ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DeeplabV3+ model after test evaluation\n",
    "deeplabv3_model_path = os.path.join(models_dir, 'deeplabv3_efficientnet_b0.pth')\n",
    "torch.save({\n",
    "    'model_state_dict': deeplabv3_model_trained.state_dict(),\n",
    "    'optimizer_state_dict': deeplabv3_optimizer.state_dict(),\n",
    "    'history': deeplabv3_history,\n",
    "    'test_acc': deeplabv3_test_acc,\n",
    "    'test_loss': deeplabv3_test_loss\n",
    "}, deeplabv3_model_path)\n",
    "print(f\"DeeplabV3+ model saved to {deeplabv3_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafe165f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DeeplabV3+ model\n",
    "def load_deeplabv3_model(model_path):\n",
    "    base_model = EfficientNet.from_pretrained('efficientnet-b0')  # We need a base model for DeeplabV3+\n",
    "    model = DeepLabV3Plus(base_model).to(device)\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    history = checkpoint['history']\n",
    "    test_acc = checkpoint['test_acc']\n",
    "    test_loss = checkpoint['test_loss']\n",
    "    print(f\"Loaded DeeplabV3+ model with test accuracy: {test_acc:.4f}\")\n",
    "    return model, history, test_acc, test_loss\n",
    "\n",
    "# Example usage:\n",
    "loaded_deeplabv3_model, loaded_deeplabv3_history, loaded_deeplabv3_acc, loaded_deeplabv3_loss = load_deeplabv3_model(deeplabv3_model_path)\n",
    "# The loaded model can now be used for inference"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
