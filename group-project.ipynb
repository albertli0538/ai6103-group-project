{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c2e861f",
   "metadata": {},
   "source": [
    "# EfficientNet-B0 Experimentation on Cityscapes Dataset\n",
    "\n",
    "This notebook implements a series of experiments to evaluate and improve the performance of EfficientNet-B0 on the Cityscapes dataset.\n",
    "\n",
    "## Overview\n",
    "\n",
    "1. **Baseline Experiment**: Train EfficientNet-B0 with standard settings\n",
    "2. **Modified Models**:\n",
    "   - Add CBAM (Convolutional Block Attention Module)\n",
    "   - Switch to Mish activation function\n",
    "   - Add DeeplabV3+ segmentation head\n",
    "3. **Comparative Analysis**: Compare and analyze the results across all models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074fc78c",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "First, let's import all necessary libraries for our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741a5961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torchvision\n",
    "from torchvision import transforms, models\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d327b0",
   "metadata": {},
   "source": [
    "## 2. Data Preparation\n",
    "\n",
    "### 2.1 Loading Cityscapes Dataset\n",
    "\n",
    "We'll load the Cityscapes dataset and prepare it for our experiments. First, we need to clone the Cityscapes repository to access the scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109dceec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the Cityscapes repository if not already present\n",
    "%git clone https://github.com/mcordts/cityscapesScripts.git\n",
    "%pip install -e cityscapesScripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0bbac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Cityscapes helper functions\n",
    "from cityscapesscripts.helpers.labels import trainId2label, id2label\n",
    "\n",
    "# Define data transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # EfficientNet-B0 input size\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Define path to the Cityscapes dataset\n",
    "# Update this path to where your Cityscapes data is located\n",
    "cityscapes_root = 'path/to/cityscapes'\n",
    "\n",
    "# Create dataset using built-in Cityscapes dataset class from torchvision\n",
    "from torchvision.datasets import Cityscapes\n",
    "\n",
    "# Load the dataset\n",
    "train_dataset = Cityscapes(\n",
    "    root=cityscapes_root,\n",
    "    split='train',\n",
    "    mode='fine',\n",
    "    target_type='semantic',\n",
    "    transform=train_transform,\n",
    "    target_transform=None\n",
    ")\n",
    "\n",
    "val_dataset = Cityscapes(\n",
    "    root=cityscapes_root,\n",
    "    split='val',\n",
    "    mode='fine',\n",
    "    target_type='semantic',\n",
    "    transform=val_test_transform,\n",
    "    target_transform=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f34559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display dataset information\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "\n",
    "# Split validation into validation and test sets\n",
    "val_size = int(len(val_dataset) * 0.5)\n",
    "test_size = len(val_dataset) - val_size\n",
    "val_dataset, test_dataset = random_split(val_dataset, [val_size, test_size])\n",
    "\n",
    "print(f\"After splitting - Validation dataset size: {len(val_dataset)}\")\n",
    "print(f\"After splitting - Test dataset size: {len(test_dataset)}\")\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Show a sample image from the dataset\n",
    "def show_sample(dataset, idx=0):\n",
    "    img, label = dataset[idx]\n",
    "    \n",
    "    # Denormalize the image\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406])\n",
    "    std = torch.tensor([0.229, 0.224, 0.225])\n",
    "    img_denorm = img * std[:, None, None] + mean[:, None, None]\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(img_denorm.permute(1, 2, 0).numpy())\n",
    "    plt.title('Image')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(label)\n",
    "    plt.title('Segmentation Mask')\n",
    "    plt.show()\n",
    "\n",
    "show_sample(train_dataset, idx=np.random.randint(len(train_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9eef666",
   "metadata": {},
   "source": [
    "## 3. Baseline Model: EfficientNet-B0\n",
    "\n",
    "We'll now set up our baseline model using EfficientNet-B0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08d5ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientNetB0Classifier(nn.Module):\n",
    "    def __init__(self, num_classes=19):  # Cityscapes has 19 classes with trainId\n",
    "        super(EfficientNetB0Classifier, self).__init__()\n",
    "        # Load the pre-trained EfficientNet-B0 model\n",
    "        self.efficient_net = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "        \n",
    "        # Replace the classifier with a new one for the number of classes in Cityscapes\n",
    "        in_features = self.efficient_net._fc.in_features\n",
    "        self.efficient_net._fc = nn.Linear(in_features, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.efficient_net(x)\n",
    "\n",
    "# Initialize the baseline model\n",
    "baseline_model = EfficientNetB0Classifier().to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(baseline_model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d1ab47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    processed_data = 0\n",
    "    \n",
    "    for inputs, labels in tqdm(dataloader, desc=\"Training\"):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "        processed_data += inputs.size(0)\n",
    "        \n",
    "    train_loss = running_loss / processed_data\n",
    "    train_acc = running_corrects.double() / processed_data\n",
    "    \n",
    "    return train_loss, train_acc.item()\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    processed_size = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "            processed_size += inputs.size(0)\n",
    "    \n",
    "    eval_loss = running_loss / processed_size\n",
    "    eval_acc = running_corrects.double() / processed_size\n",
    "    \n",
    "    return eval_loss, eval_acc.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab98fb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop for baseline model\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=10):\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "        \n",
    "        # Train phase\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        print(f'Train Loss: {train_loss:.4f} Acc: {train_acc:.4f}')\n",
    "        \n",
    "        # Validation phase\n",
    "        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "        print(f'Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}')\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Deep copy the model if it's the best\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        \n",
    "        # Update history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    # Load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, history\n",
    "\n",
    "import copy\n",
    "\n",
    "# Train the baseline model\n",
    "baseline_model_trained, baseline_history = train_model(\n",
    "    baseline_model, \n",
    "    train_loader, \n",
    "    val_loader, \n",
    "    criterion, \n",
    "    optimizer, \n",
    "    scheduler,\n",
    "    num_epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09aef973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the baseline model on test set\n",
    "test_loss, test_acc = evaluate(baseline_model_trained, test_loader, criterion, device)\n",
    "print(f'Baseline Model - Test Loss: {test_loss:.4f} Acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2e6996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the training history\n",
    "def plot_training_history(history, title):\n",
    "    epochs = range(1, len(history['train_loss'])+1)\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, history['train_loss'], 'bo-', label='Training Loss')\n",
    "    plt.plot(epochs, history['val_loss'], 'ro-', label='Validation Loss')\n",
    "    plt.title(f'{title} - Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, history['train_acc'], 'bo-', label='Training Accuracy')\n",
    "    plt.plot(epochs, history['val_acc'], 'ro-', label='Validation Accuracy')\n",
    "    plt.title(f'{title} - Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize baseline model training history\n",
    "plot_training_history(baseline_history, 'Baseline EfficientNet-B0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561b867a",
   "metadata": {},
   "source": [
    "## 4. Modified Models\n",
    "\n",
    "### 4.1 EfficientNet-B0 with CBAM (Convolutional Block Attention Module)\n",
    "\n",
    "CBAM enhances the representational power by focusing on important features and suppressing unnecessary ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83bc217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing CBAM \n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "           \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc(self.avg_pool(x))\n",
    "        max_out = self.fc(self.max_pool(x))\n",
    "        out = avg_out + max_out\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        concat = torch.cat([avg_out, max_out], dim=1)\n",
    "        out = self.conv(concat)\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=16, kernel_size=7):\n",
    "        super(CBAM, self).__init__()\n",
    "        self.channel_att = ChannelAttention(in_planes, ratio)\n",
    "        self.spatial_att = SpatialAttention(kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x * self.channel_att(x)\n",
    "        x = x * self.spatial_att(x)\n",
    "        return x\n",
    "\n",
    "# EfficientNet-B0 with CBAM attention\n",
    "class EfficientNetB0WithCBAM(nn.Module):\n",
    "    def __init__(self, num_classes=19):\n",
    "        super(EfficientNetB0WithCBAM, self).__init__()\n",
    "        self.efficient_net = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "        in_features = self.efficient_net._fc.in_features\n",
    "        \n",
    "        # Add CBAM at the end of feature extraction\n",
    "        self.cbam = CBAM(in_features)\n",
    "        \n",
    "        # Replace classifier\n",
    "        self.efficient_net._fc = nn.Linear(in_features, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Extract features before the final FC layer\n",
    "        features = self.efficient_net.extract_features(x)\n",
    "        \n",
    "        # Apply CBAM\n",
    "        features_with_attention = self.cbam(features)\n",
    "        \n",
    "        # Continue with the rest of EfficientNet forward pass\n",
    "        x = self.efficient_net._avg_pooling(features_with_attention)\n",
    "        x = x.flatten(start_dim=1)\n",
    "        x = self.efficient_net._dropout(x)\n",
    "        x = self.efficient_net._fc(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Initialize the CBAM model\n",
    "cbam_model = EfficientNetB0WithCBAM().to(device)\n",
    "\n",
    "# Define optimizer for CBAM model\n",
    "cbam_optimizer = optim.SGD(cbam_model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
    "cbam_scheduler = optim.lr_scheduler.ReduceLROnPlateau(cbam_optimizer, 'min', patience=3, factor=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00cefa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the CBAM model\n",
    "cbam_model_trained, cbam_history = train_model(\n",
    "    cbam_model, \n",
    "    train_loader, \n",
    "    val_loader, \n",
    "    criterion, \n",
    "    cbam_optimizer, \n",
    "    cbam_scheduler,\n",
    "    num_epochs=10\n",
    ")\n",
    "\n",
    "# Evaluate CBAM model on test set\n",
    "cbam_test_loss, cbam_test_acc = evaluate(cbam_model_trained, test_loader, criterion, device)\n",
    "print(f'CBAM Model - Test Loss: {cbam_test_loss:.4f} Acc: {cbam_test_acc:.4f}')\n",
    "\n",
    "# Visualize CBAM model training history\n",
    "plot_training_history(cbam_history, 'EfficientNet-B0 with CBAM')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2709a9f",
   "metadata": {},
   "source": [
    "### 4.2 EfficientNet-B0 with Mish Activation Function\n",
    "\n",
    "Mish is a self-regularized non-monotonic activation function that often outperforms ReLU and its variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735b8242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing Mish activation\n",
    "class Mish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * torch.tanh(F.softplus(x))\n",
    "\n",
    "# EfficientNet-B0 with Mish activation\n",
    "class EfficientNetB0WithMish(nn.Module):\n",
    "    def __init__(self, num_classes=19):\n",
    "        super(EfficientNetB0WithMish, self).__init__()\n",
    "        self.efficient_net = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "        \n",
    "        # Replace all activation functions with Mish\n",
    "        self._replace_relu_with_mish(self.efficient_net)\n",
    "        \n",
    "        # Replace classifier\n",
    "        in_features = self.efficient_net._fc.in_features\n",
    "        self.efficient_net._fc = nn.Linear(in_features, num_classes)\n",
    "\n",
    "    def _replace_relu_with_mish(self, model):\n",
    "        for name, module in model.named_children():\n",
    "            if isinstance(module, nn.ReLU):\n",
    "                setattr(model, name, Mish())\n",
    "            else:\n",
    "                self._replace_relu_with_mish(module)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.efficient_net(x)\n",
    "\n",
    "# Initialize the Mish model\n",
    "mish_model = EfficientNetB0WithMish().to(device)\n",
    "\n",
    "# Define optimizer for Mish model\n",
    "mish_optimizer = optim.SGD(mish_model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
    "mish_scheduler = optim.lr_scheduler.ReduceLROnPlateau(mish_optimizer, 'min', patience=3, factor=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2611dce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Mish model\n",
    "mish_model_trained, mish_history = train_model(\n",
    "    mish_model, \n",
    "    train_loader, \n",
    "    val_loader, \n",
    "    criterion, \n",
    "    mish_optimizer, \n",
    "    mish_scheduler,\n",
    "    num_epochs=10\n",
    ")\n",
    "\n",
    "# Evaluate Mish model on test set\n",
    "mish_test_loss, mish_test_acc = evaluate(mish_model_trained, test_loader, criterion, device)\n",
    "print(f'Mish Model - Test Loss: {mish_test_loss:.4f} Acc: {mish_test_acc:.4f}')\n",
    "\n",
    "# Visualize Mish model training history\n",
    "plot_training_history(mish_history, 'EfficientNet-B0 with Mish')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885c65b4",
   "metadata": {},
   "source": [
    "### 4.3 EfficientNet-B0 with DeeplabV3+ Segmentation Head\n",
    "\n",
    "DeepLabV3+ is a semantic segmentation architecture that combines atrous convolution with encoder-decoder structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4340582d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing DeeplabV3+ segmentation head\n",
    "class ASPP(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, rates=[6, 12, 18]):\n",
    "        super(ASPP, self).__init__()\n",
    "        \n",
    "        self.aspp = nn.ModuleList()\n",
    "        \n",
    "        # 1x1 convolution\n",
    "        self.aspp.append(nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU()\n",
    "        ))\n",
    "        \n",
    "        # Atrous convolutions\n",
    "        for rate in rates:\n",
    "            self.aspp.append(nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 3, padding=rate, dilation=rate, bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU()\n",
    "            ))\n",
    "        \n",
    "        # Global average pooling\n",
    "        self.global_avg_pool = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(in_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Output layer\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Conv2d(out_channels * (len(rates) + 2), out_channels, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        size = x.size()[2:]\n",
    "        \n",
    "        outputs = []\n",
    "        for module in self.aspp:\n",
    "            outputs.append(module(x))\n",
    "        \n",
    "        # Process global average pooling branch\n",
    "        gap_output = self.global_avg_pool(x)\n",
    "        gap_output = F.interpolate(gap_output, size=size, mode='bilinear', align_corners=True)\n",
    "        outputs.append(gap_output)\n",
    "        \n",
    "        # Concatenate and process through output layer\n",
    "        x = torch.cat(outputs, dim=1)\n",
    "        return self.output(x)\n",
    "\n",
    "class DeepLabV3Plus(nn.Module):\n",
    "    def __init__(self, base_model, num_classes=19, output_stride=16):\n",
    "        super(DeepLabV3Plus, self).__init__()\n",
    "        self.backbone = base_model\n",
    "        in_features = self.backbone._fc.in_features\n",
    "        \n",
    "        # ASPP module\n",
    "        self.aspp = ASPP(in_features, 256)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, num_classes, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        input_size = x.size()[2:]\n",
    "        \n",
    "        # Extract features\n",
    "        features = self.backbone.extract_features(x)\n",
    "        \n",
    "        # Apply ASPP\n",
    "        x = self.aspp(features)\n",
    "        \n",
    "        # Decoder\n",
    "        x = self.decoder(x)\n",
    "        \n",
    "        # Upsampling to original size\n",
    "        x = F.interpolate(x, size=input_size, mode='bilinear', align_corners=True)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Initialize the DeepLabV3+ model\n",
    "base_model = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "deeplabv3_model = DeepLabV3Plus(base_model).to(device)\n",
    "\n",
    "# Define optimizer for DeepLabV3+ model\n",
    "deeplabv3_optimizer = optim.SGD(deeplabv3_model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
    "deeplabv3_scheduler = optim.lr_scheduler.ReduceLROnPlateau(deeplabv3_optimizer, 'min', patience=3, factor=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2415bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the DeepLabV3+ model\n",
    "deeplabv3_model_trained, deeplabv3_history = train_model(\n",
    "    deeplabv3_model, \n",
    "    train_loader, \n",
    "    val_loader, \n",
    "    criterion, \n",
    "    deeplabv3_optimizer, \n",
    "    deeplabv3_scheduler,\n",
    "    num_epochs=10\n",
    ")\n",
    "\n",
    "# Evaluate DeepLabV3+ model on test set\n",
    "deeplabv3_test_loss, deeplabv3_test_acc = evaluate(deeplabv3_model_trained, test_loader, criterion, device)\n",
    "print(f'DeepLabV3+ Model - Test Loss: {deeplabv3_test_loss:.4f} Acc: {deeplabv3_test_acc:.4f}')\n",
    "\n",
    "# Visualize DeepLabV3+ model training history\n",
    "plot_training_history(deeplabv3_history, 'EfficientNet-B0 with DeepLabV3+')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec6bde4",
   "metadata": {},
   "source": [
    "## 5. Results Comparison and Analysis\n",
    "\n",
    "Let's compare the performance of all model variants across various metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861aa14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "results = {\n",
    "    'Model': ['Baseline', 'With CBAM', 'With Mish', 'With DeepLabV3+'],\n",
    "    'Test Accuracy': [test_acc, cbam_test_acc, mish_test_acc, deeplabv3_test_acc],\n",
    "    'Test Loss': [test_loss, cbam_test_loss, mish_test_loss, deeplabv3_test_loss]\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"Model Performance Comparison:\")\n",
    "display(results_df)\n",
    "\n",
    "# Visualize comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "x = np.arange(len(results['Model']))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, results['Test Accuracy'], width, label='Test Accuracy')\n",
    "plt.bar(x + width/2, results['Test Loss'], width, label='Test Loss')\n",
    "\n",
    "plt.xlabel('Model')\n",
    "plt.title('Performance Comparison of EfficientNet-B0 Variants')\n",
    "plt.xticks(x, results['Model'], rotation=45)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516c3d46",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
